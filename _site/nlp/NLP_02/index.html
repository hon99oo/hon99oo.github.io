<!doctype html>
<!--
  Minimal Mistakes Jekyll Theme 4.24.0 by Michael Rose
  Copyright 2013-2020 Michael Rose - mademistakes.com | @mmistakes
  Free for personal and commercial use under the MIT license
  https://github.com/mmistakes/minimal-mistakes/blob/master/LICENSE
-->
<html lang="ko" class="no-js">
  <head>
    <meta charset="utf-8">

<!-- begin _includes/seo.html --><title>[NLP/Paper]MASS:Masked Sequence to Sequence pre-training for Language Generation 논문 리뷰 - 홍구의 개발 블로그</title>
<meta name="description" content="NLP 랩실에서의 논문 리딩이었던 것…2">


  <meta name="author" content="honggoo">
  
  <meta property="article:author" content="honggoo">
  


<meta property="og:type" content="article">
<meta property="og:locale" content="ko_KR">
<meta property="og:site_name" content="홍구의 개발 블로그">
<meta property="og:title" content="[NLP/Paper]MASS:Masked Sequence to Sequence pre-training for Language Generation 논문 리뷰">
<meta property="og:url" content="http://localhost:4000/nlp/NLP_02/">


  <meta property="og:description" content="NLP 랩실에서의 논문 리딩이었던 것…2">



  <meta property="og:image" content="https://images.unsplash.com/photo-1501785888041-af3ef285b470?ixlib=rb-1.2.1&amp;ixid=eyJhcHBfaWQiOjEyMDd9&amp;auto=format&amp;fit=crop&amp;w=1350&amp;q=80">





  <meta property="article:published_time" content="2022-01-23T00:00:00+09:00">





  

  


<link rel="canonical" href="http://localhost:4000/nlp/NLP_02/">




<script type="application/ld+json">
  {
    "@context": "https://schema.org",
    
      "@type": "Person",
      "name": "honggoo",
      "url": "http://localhost:4000/"
    
  }
</script>







<!-- end _includes/seo.html -->



  <link href="/feed.xml" type="application/atom+xml" rel="alternate" title="홍구의 개발 블로그 Feed">


<!-- https://t.co/dKP3o1e -->
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<script>
  document.documentElement.className = document.documentElement.className.replace(/\bno-js\b/g, '') + ' js ';
</script>

<!-- For all browsers -->
<link rel="stylesheet" href="/assets/css/main.css">
<link rel="preload" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css" as="style" onload="this.onload=null;this.rel='stylesheet'">
<noscript><link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5/css/all.min.css"></noscript>



    <!-- start custom head snippets -->

<!-- insert favicons. use https://realfavicongenerator.net/ -->

<!-- end custom head snippets -->

  </head>

  <body class="layout--single">
    <nav class="skip-links">
  <ul>
    <li><a href="#site-nav" class="screen-reader-shortcut">Skip to primary navigation</a></li>
    <li><a href="#main" class="screen-reader-shortcut">Skip to content</a></li>
    <li><a href="#footer" class="screen-reader-shortcut">Skip to footer</a></li>
  </ul>
</nav>

    <!--[if lt IE 9]>
<div class="notice--danger align-center" style="margin: 0;">You are using an <strong>outdated</strong> browser. Please <a href="https://browsehappy.com/">upgrade your browser</a> to improve your experience.</div>
<![endif]-->

    

<div class="masthead">
  <div class="masthead__inner-wrap">
    <div class="masthead__menu">
      <nav id="site-nav" class="greedy-nav">
        
        <a class="site-title" href="/">
          홍구의 개발 블로그
          
        </a>
        <ul class="visible-links"><li class="masthead__menu-item">
              <a href="/about_me/">About Me</a>
            </li><li class="masthead__menu-item">
              <a href="/categories/">Category</a>
            </li><li class="masthead__menu-item">
              <a href="/tags/">Tag</a>
            </li><li class="masthead__menu-item">
              <a href="/year-archive/">Archive</a>
            </li></ul>
        
        <button class="search__toggle" type="button">
          <span class="visually-hidden">Toggle search</span>
          <i class="fas fa-search"></i>
        </button>
        
        <button class="greedy-nav__toggle hidden" type="button">
          <span class="visually-hidden">토글 메뉴</span>
          <div class="navicon"></div>
        </button>
        <ul class="hidden-links hidden"></ul>
      </nav>
    </div>
  </div>
</div>


    <div class="initial-content">
      
  







<div class="page__hero--overlay"
  style=" background-image: linear-gradient(rgba(0, 0, 0, 0.5), rgba(0, 0, 0, 0.5)), url('https://images.unsplash.com/photo-1501785888041-af3ef285b470?ixlib=rb-1.2.1&ixid=eyJhcHBfaWQiOjEyMDd9&auto=format&fit=crop&w=1350&q=80');"
>
  
    <div class="wrapper">
      <h1 id="page-title" class="page__title" itemprop="headline">
        
          [NLP/Paper]MASS:Masked Sequence to Sequence pre-training for Language Generation 논문 리뷰

        
      </h1>
      
        <p class="page__lead">NLP 랩실에서의 논문 리딩이었던 것…2
</p>
      
      

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 분 소요
        
      </span>
    
  </p>


      
      
    </div>
  
  
</div>





<div id="main" role="main">
  
  <div class="sidebar sticky">
  


<div itemscope itemtype="https://schema.org/Person" class="h-card">

  
    <div class="author__avatar">
      <a href="http://localhost:4000/">
        <img src="/assets/img/avatar2.png" alt="honggoo" itemprop="image" class="u-photo">
      </a>
    </div>
  

  <div class="author__content">
    <h3 class="author__name p-name" itemprop="name">
      <a class="u-url" rel="me" href="http://localhost:4000/" itemprop="url">honggoo</a>
    </h3>
    
      <div class="author__bio p-note" itemprop="description">
        <p>본격적인 개발 블로그를 시작해 볼까!</p>

      </div>
    
  </div>

  <div class="author__urls-wrapper">
    <button class="btn btn--inverse">팔로우</button>
    <ul class="author__urls social-icons">
      
        <li itemprop="homeLocation" itemscope itemtype="https://schema.org/Place">
          <i class="fas fa-fw fa-map-marker-alt" aria-hidden="true"></i> <span itemprop="name" class="p-locality">Seoul, South Korea</span>
        </li>
      

      
        
          
            <li><a href="cocoti5797@gmail.com" rel="nofollow noopener noreferrer me"><i class="fas fa-fw fa-envelope-square" aria-hidden="true"></i><span class="label">Email</span></a></li>
          
        
          
        
          
        
          
        
          
            <li><a href="https://github.com/hon99oo" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-github" aria-hidden="true"></i><span class="label">GitHub</span></a></li>
          
        
          
            <li><a href="https://instagram.com/hon99oo" rel="nofollow noopener noreferrer me"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i><span class="label">Instagram</span></a></li>
          
        
      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      

      <!--
  <li>
    <a href="http://link-to-whatever-social-network.com/user/" itemprop="sameAs" rel="nofollow noopener noreferrer me">
      <i class="fas fa-fw" aria-hidden="true"></i> Custom Social Profile Link
    </a>
  </li>
-->
    </ul>
  </div>
</div>
  
  </div>



  <article class="page h-entry" itemscope itemtype="https://schema.org/CreativeWork">
    <meta itemprop="headline" content="[NLP/Paper]MASS:Masked Sequence to Sequence pre-training for Language Generation 논문 리뷰">
    <meta itemprop="description" content="NLP 랩실에서의 논문 리딩이었던 것…2">
    <meta itemprop="datePublished" content="2022-01-23T00:00:00+09:00">
    

    <div class="page__inner-wrap">
      

      <section class="page__content e-content" itemprop="text">
        
          <aside class="sidebar__right sticky">
            <nav class="toc">
              <header><h4 class="nav__title"><i class="fas fa-bars"></i> table of content</h4></header>
              <ul class="toc__menu"><li><a href="#introduction">Introduction</a></li><li><a href="#encoder-attention-decoder">Encoder-Attention-Decoder</a></li><li><a href="#related-works">Related Works</a></li><li><a href="#massmasked-sequence-to-sequence-pre-training">MASS:Masked Sequence to Sequence Pre-training</a></li><li><a href="#experiments">Experiments</a></li><li><a href="#experimentunsupervised-machine-translation">Experiment:Unsupervised Machine Translation</a></li><li><a href="#experimentlow-resource-translation">Experiment:Low-resource Translation</a></li><li><a href="#experimentabstractive-summarization">Experiment:Abstractive Summarization</a></li><li><a href="#experimentconversational-response-generation">Experiment:Conversational Response Generation</a></li><li><a href="#the-probability-formulation">The probability formulation</a></li><li><a href="#couclusion">Couclusion</a></li></ul>

            </nav>
          </aside>
        
        <h1 id="introduction">Introduction</h1>
<ul>
  <li>Pre-training은 최근 NLP 분야에서 활발한 연구가 이루어지는 주제 중 하나</li>
  <li>Google의 Bert와 XLNet은 자연어 이해(NLU) Task에서 큰 성공을 거둔 모델</li>
  <li>NLU Task 외에 Sequence to Sequence를 기반으로 하는 자연어 생성 Task가 있음
    <ul>
      <li>Machine Translation, Abstract Text Summarization</li>
    </ul>
  </li>
  <li>자연어 생성 Task는 Encoder - Attention - Decoder 구조의 모델들이 사용됨</li>
</ul>

<h1 id="encoder-attention-decoder">Encoder-Attention-Decoder</h1>
<p><img src="../../assets/img/nlp/02/img.png" width="100%" height="100%" /></p>
<ul>
  <li>Encoder는 Source sequence X를 input으로 받은 후, 이를 hidden representations sequence로 변형</li>
  <li>Decoder는 Encoder로부터 hidden representations sequence를 전달 받아 Target Sequence Y를 생성하며, 이 때 Attention mechanism이 학습한 Input에 대한 정보를 함께 참조</li>
</ul>

<p><br /><br /></p>
<h1 id="related-works">Related Works</h1>
<p><img src="../../assets/img/nlp/02/img_1.png" width="100%" height="100%" /></p>
<ul>
  <li>Bert와 XLNet은 Language Understanding을 위해 Encoder를 Pre-train</li>
  <li>GPT는 Language Modeling을 위해 Decoder를 Pre-train</li>
  <li>이전 모델들은 Encoder, Attention, Decoder를 함께 Pre-train하지 못했음</li>
</ul>

<p><br /><br /></p>
<h1 id="massmasked-sequence-to-sequence-pre-training">MASS:Masked Sequence to Sequence Pre-training</h1>
<p><img src="../../assets/img/nlp/02/img_2.png" width="100%" height="100%" /></p>
<ul>
  <li>새로운 모델 MASS는 Masked Sequence to Sequence Pre-training의 약자</li>
  <li>Input에서 K개의 토큰 Fragment를 임의로 지정해 Masking</li>
  <li>마스킹 된 Fragment를 Encoder-Attention-Decoder를 거쳐 예측하도록 훈련시킴</li>
  <li>Encoder에서 Masking되지 않은 토큰들이 Decoder에서 Masking됨에 따라, Decoder는 Encoder가 제공한 hidden representation과 Attention 정보만을 참고하여 Masking된 토큰들을 예측해야함, 이를 통해 Encoder-Attention-Decoder가 함께 Pre-train될 수 있는 환경을 제공</li>
  <li>Encoder는 Encoder에서 Masking되지 않은 토큰들의 정보를 잘 추출할 수 있도록 학습, 이를 통해 Language Understanding 능력 개선</li>
  <li>Decoder는 Encoder에서 Maksing된 토큰들에 대한 예측을 연속적으로 수행해야 하기 때문에 Language Modeling 능력을 학습하게 됨</li>
</ul>

<p><img src="../../assets/img/nlp/02/img_3.png" width="100%" height="100%" /></p>
<ul>
  <li>K는 Encoder에서 Masking되는 토큰 개수를 결정하는 하이퍼 파라미터</li>
  <li>하이퍼 파라미터 K를 조정함으로써 MASS는 BERT의 Masked LM과 GPT의 Standard LM을 모두 구현 가능</li>
  <li>BERT Masked LM
    <ul>
      <li>K=1일 때, Encoder에서는 하나의 토큰이 Masking되고, Decoder는 Masking된 하나의 토큰을 예측해야 하므로 BERT의 Masked LM과 같아짐</li>
    </ul>
  </li>
  <li>GPT Standard LM
    <ul>
      <li>하이퍼 파라미터 K가 Input 문장의 전체 길이인 m과 같을 때 Encoder의 모든 토큰들이 Masking됨</li>
      <li>Decoder는 Encoder로부터 어떠한 정보도 부여 받지 않은 채 모든 토큰을 예측해야 하므로, GPT의 Standard LM과 같아짐</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/02/img_4.png" width="100%" height="100%" /></p>
<ul>
  <li>m - Input sequence의 전체길이</li>
  <li>u - Maskinge된 Fragment의 시작점</li>
  <li>v - Masking된 Fragment의 끝점</li>
  <li>X^u - u부터 v까지의 fragment</li>
  <li>X^\u:v - u부터 v까지 Making된 Input Sequence</li>
</ul>

<p><br /><br /></p>
<h1 id="experiments">Experiments</h1>
<ul>
  <li>Model Configuration
    <ul>
      <li>1024 embeding/hidden size, 4096 feed-forward filter size를 가진 6-layer encoder, 6-layer decoder로 구성된 기본 모델 구조 Transformer</li>
    </ul>
  </li>
  <li>Datasets
    <ul>
      <li>2007 - 2017 WMT News Crawl datasets 190M English, 62M French, German 270M</li>
      <li>MASS의 효과 검증을 위한 low-resource language Romanian</li>
    </ul>
  </li>
  <li>Pre-training Details
    <ul>
      <li>Fragment length K의 길이를 문장에 있는 총 토큰 수의 약 50%로 설정하고 정확성의 변화를 비교하기 위하여 K를 바꿔가며 실험</li>
    </ul>
  </li>
</ul>

<p><br /><br /></p>
<h1 id="experimentunsupervised-machine-translation">Experiment:Unsupervised Machine Translation</h1>
<p><img src="../../assets/img/nlp/02/img_5.png" width="100%" height="100%" /></p>
<ul>
  <li>Unsupervised Machine Translation Task에 있어 이전 모델들, 최근까지 SOTA였던 Facebook의 XLM모델과 MASS의 성능 비교를 수행</li>
  <li>Facebook의 XLM은 Encoder와 Decoder를 각각 Bert의 Masked LM과 standard LM으로 Pre-train 시킨 모델</li>
  <li>MASS는 6개의 Machine Translation Task에 있어 XLM을 능가하는 성능, SOTA 기록</li>
</ul>

<p><br /><br /></p>

<h1 id="experimentlow-resource-translation">Experiment:Low-resource Translation</h1>
<p><img src="../../assets/img/nlp/02/img_6.png" width="100%" height="100%" /></p>
<ul>
  <li>Low-resource Machine Translation : bilingual 트레이닝 데이터셋이 부족한 환경에서의 기계번역</li>
  <li>Englisgh-Fench, English-German, English-Romanian 데이터 셋을 10K, 100K, 1M의 사이즈로 늘려가며 Low-resource 환경에서의 Machine Translation 테스트</li>
  <li>MASS는 모든 스케일에서 Low-resource MT의 baseline 성능을 능가</li>
  <li>데이터셋의 사이즈가 작을수록 더욱 두드러지게 나타남</li>
</ul>

<p><br /><br /></p>

<h1 id="experimentabstractive-summarization">Experiment:Abstractive Summarization</h1>
<p><img src="../../assets/img/nlp/02/img_7.png" width="100%" height="100%" /></p>
<ul>
  <li>Pre-trained Bert를 인코더로 Pre-trained Language Model을 Decoder로 사용한 BERT+LM 모델과 DAE(Denoising Auto-Encoder), MASS의 Abstractive Summarization 성능을 Gigaword Corpus에 대해 비교</li>
  <li>MASS는 BERT+LM과 DAE 두 모델의 Abstractive Summarization 성능을 모두 능가</li>
</ul>

<p><br /><br /></p>

<h1 id="experimentconversational-response-generation">Experiment:Conversational Response Generation</h1>
<p><img src="../../assets/img/nlp/02/img_8.png" width="100%" height="100%" /></p>
<ul>
  <li>Abstractive Summarization 성능 비교에 사용되었던 BERT+LM 모델과 Baseline, MASS의 Conversational Response Generation 성능을 Cornell Movie Dialog Corpus에 대해 비교</li>
  <li>MASS가 BERT+LM 모델과 Baseline 보다 낮은 Perplexity를 기록하며 더 좋은 성능을 보여줌</li>
</ul>

<p><br /><br /></p>

<h1 id="the-probability-formulation">The probability formulation</h1>
<p><img src="../../assets/img/nlp/02/img_9.png" width="100%" height="100%" /></p>
<ul>
  <li>(a) 영어-프랑스 번역의 영어, (b) 영어-프랑스 번역의 프랑스어, (c) 영어-프랑스 번역의 BLUE score (d) Text Summarization의 ROUGE score, (e) conversational response generation의 PPL</li>
  <li>하이퍼 파라미터 K를 다르게 설정해가며, MASS의 성능에 대한 다양한 실험 결과</li>
  <li>실험을 통해 K가 문장의 절반 정도 크기에 해당할 때, downstream task에서 가장 좋은 성능을 보임을 알게 됨
    <ul>
      <li>문장 내 절반의 토큰을 Masking하는 것이 Encoder와 Decoder의 Pre-training에 있어 적절한 균형을 제공</li>
    </ul>
  </li>
  <li>K가 1(BERT), K가 m(GPT) 이었을 때는 downstream task에서 좋은 성능이 나오지 않음 이 이유는 MASS가 Sequence to Sequence 기반의 Language Generation Task에 이점을 지니고 있음을 반증</li>
</ul>

<p><br /><br /></p>
<h1 id="couclusion">Couclusion</h1>
<ul>
  <li>MASS는 Sequence to Sequence 기반의 다양한 Language generation Task에 있어 좋은 성능 기록</li>
  <li>Language generation에서 좋은 성능을 기록한 MASS가 BERT와 XLnet이 뛰어난 결과를 기록한 Natural Language Understanding Task에서도 좋은 성능을 보일 것인지 차후 실험해볼 예정</li>
  <li>또한 MASS 모델이 Image와 Video와 같은 NLP와 다른 도메인에서도 Sequence to Sequence 기반의 생성 Task를 수행해낼 수 있을지에 대한 추가 실험도 해볼 예정</li>
</ul>

        
      </section>

      <footer class="page__meta">
        
        
  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-tags" aria-hidden="true"></i> 태그: </strong>
    <span itemprop="keywords">
    
      <a href="/tags/#nlp" class="page__taxonomy-item p-category" rel="tag">nlp</a><span class="sep">, </span>
    
      <a href="/tags/#paper-review" class="page__taxonomy-item p-category" rel="tag">paper review</a>
    
    </span>
  </p>




  


  

  <p class="page__taxonomy">
    <strong><i class="fas fa-fw fa-folder-open" aria-hidden="true"></i> 카테고리: </strong>
    <span itemprop="keywords">
    
      <a href="/categories/#nlp" class="page__taxonomy-item p-category" rel="tag">NLP</a>
    
    </span>
  </p>


        

  <p class="page__date"><strong><i class="fas fa-fw fa-calendar-alt" aria-hidden="true"></i> 업데이트:</strong> <time class="dt-published" datetime="2022-01-23T00:00:00+09:00">January 23, 2022</time></p>

      </footer>

      <section class="page__share">
  
    <h4 class="page__share-title">공유하기</h4>
  

  <a href="https://twitter.com/intent/tweet?text=%5BNLP%2FPaper%5DMASS%3AMasked+Sequence+to+Sequence+pre-training+for+Language+Generation+%EB%85%BC%EB%AC%B8+%EB%A6%AC%EB%B7%B0%20http%3A%2F%2Flocalhost%3A4000%2Fnlp%2FNLP_02%2F" class="btn btn--twitter" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Twitter"><i class="fab fa-fw fa-twitter" aria-hidden="true"></i><span> Twitter</span></a>

  <a href="https://www.facebook.com/sharer/sharer.php?u=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2FNLP_02%2F" class="btn btn--facebook" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 Facebook"><i class="fab fa-fw fa-facebook" aria-hidden="true"></i><span> Facebook</span></a>

  <a href="https://www.linkedin.com/shareArticle?mini=true&url=http%3A%2F%2Flocalhost%3A4000%2Fnlp%2FNLP_02%2F" class="btn btn--linkedin" onclick="window.open(this.href, 'window', 'left=20,top=20,width=500,height=500,toolbar=1,resizable=0'); return false;" title="공유하기 LinkedIn"><i class="fab fa-fw fa-linkedin" aria-hidden="true"></i><span> LinkedIn</span></a>
</section>


      
  <nav class="pagination">
    
      <a href="/cs:app/csapp_04/" class="pagination--pager" title="[CS:APP] Chapter 02. 정보의 표현과 처리 (2.4 부동소수점)
">이전</a>
    
    
      <a href="/nlp/NLP_03/" class="pagination--pager" title="[NLP/Paper]ELMO - Deep contextualized word representations 논문 리뷰
">다음</a>
    
  </nav>

    </div>

    
  </article>

  
  
    <div class="page__related">
      <h2 class="page__related-title">참고</h2>
      <div class="grid__wrapper">
        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/cs:app/csapp_16/" rel="permalink">[CS:APP] Chapter 04. 프로세서 구조 (4.3 순차적 Y86-64 구현 - 4.3.4 SEQ 단계의 구현)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          3 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">CS:APP - 16
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/cs:app/csapp_15/" rel="permalink">[CS:APP] Chapter 04. 프로세서 구조 (4.3 순차적 Y86-64 구현 - 4.3.2 SEQ 하드웨어 구조)
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">CS:APP - 15
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/NLP_06/" rel="permalink">[NLP/CS224n]CS224n Lecture 17: Multitask Learning
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">NLP 랩실에서의 CS224n 강의 리뷰이었던 것…
</p>
  </article>
</div>

        
          



<div class="grid__item">
  <article class="archive__item" itemscope itemtype="https://schema.org/CreativeWork">
    
    <h2 class="archive__item-title no_toc" itemprop="headline">
      
        <a href="/nlp/NLP_05/" rel="permalink">[NLP/Paper]Deep contextualized word representations 논문 리뷰
</a>
      
    </h2>
    

  <p class="page__meta">
    

    

    
      
      

      <span class="page__meta-readtime">
        <i class="far fa-clock" aria-hidden="true"></i>
        
          2 분 소요
        
      </span>
    
  </p>


    <p class="archive__item-excerpt" itemprop="description">NLP 랩실에서의 논문 리딩이었던 것…5
</p>
  </article>
</div>

        
      </div>
    </div>
  
  
</div>
    </div>

    
      <div class="search-content">
        <div class="search-content__inner-wrap"><form class="search-content__form" onkeydown="return event.key != 'Enter';" role="search">
    <label class="sr-only" for="search">
      Enter your search term...
    </label>
    <input type="search" id="search" class="search-input" tabindex="-1" placeholder="검색어를 입력하세요..." />
  </form>
  <div id="results" class="results"></div></div>

      </div>
    

    <div id="footer" class="page__footer">
      <footer>
        <!-- start custom footer snippets -->

<!-- end custom footer snippets -->
        <div class="page__footer-follow">
  <ul class="social-icons">
    
      <li><strong>팔로우:</strong></li>
    

    
      
        
      
        
      
        
          <li><a href="https://github.com/hon99oo" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-github" aria-hidden="true"></i> GitHub</a></li>
        
      
        
      
        
      
        
          <li><a href="https://instagram.com/hon99oo" rel="nofollow noopener noreferrer"><i class="fab fa-fw fa-instagram" aria-hidden="true"></i> Instagram</a></li>
        
      
    

    
      <li><a href="/feed.xml"><i class="fas fa-fw fa-rss-square" aria-hidden="true"></i> 피드</a></li>
    
  </ul>
</div>

<div class="page__footer-copyright">&copy; 2022 honggoo. Powered by <a href="https://jekyllrb.com" rel="nofollow">Jekyll</a> &amp; <a href="https://mademistakes.com/work/minimal-mistakes-jekyll-theme/" rel="nofollow">Minimal Mistakes</a>.</div>

      </footer>
    </div>

    
  <script src="/assets/js/main.min.js"></script>




<script src="/assets/js/lunr/lunr.min.js"></script>
<script src="/assets/js/lunr/lunr-store.js"></script>
<script src="/assets/js/lunr/lunr-en.js"></script>




    
  <script>
    var disqus_config = function () {
      this.page.url = "http://localhost:4000/nlp/NLP_02/";  /* Replace PAGE_URL with your page's canonical URL variable */
      this.page.identifier = "/nlp/NLP_02"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */
    };
    (function() { /* DON'T EDIT BELOW THIS LINE */
      var d = document, s = d.createElement('script');
      s.src = 'https://honggoo.disqus.com/embed.js';
      s.setAttribute('data-timestamp', +new Date());
      (d.head || d.body).appendChild(s);
    })();
  </script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>


  





  </body>
</html>
