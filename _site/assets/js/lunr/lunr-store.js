var store = [{
        "title": "블로그를 다시 시작해 보자",
        "excerpt":"4학년 1학기 졸업작품을 3월부터 6월까지 열심히 달렸다. 졸업작품을 끝마치고 나니 현업의 욕심이 생겨 열심히 인턴 지원을 진행하였다. 그렇게 자소서를 쓰고 대학생활을 하며 잰힝했던 포트폴리오를 정리하고 7월 11일에 ‘스페이스워크’라는 회사에 인턴 포지션으로 입사하게 되었다. 인턴 포지션에 입사한 이후로 코딩에 대한 열정이 팍 식은거 같다. 회사일은 내가 졸업작품을 했을 때 처럼 큰 열정을 쏟아 붓지 못한 채 시키는 일만 하는 수동적 인간이 되었다. 그렇게 거의 5개월을 따로 공부도 하지 않고 열정이 없는 채로 살아왔던 것 같다. 당장 내 앞에 펼쳐져 있는 숙제들도 해결하지 않고 뒤로 미뤄두고 기상 - 회사 일 쪼금 - 게임 - 취침의 쳇바퀴의 반복적 인생을 살아왔다.   인턴 포지션 종료일은 1월 11일이다. 이후 나는 졸업요건을 하나 충족하지 않아 초과학기를 해야하며 다음 취업을 준비해야 한다. 인턴 포지션 종료까지는 약 2개월 정도 남았다. 지금은 욕심이지만, 인턴 종료 이후 곧바로 다른 회사의 인턴 포지션으로 들어가거나 현재 있는 회사에 잔류 하는 것이 목표이다. 현재 다니고 있는 회사에서 열심히 하지 못했던 이유는 나에게 목표의식이 조금 사라졌다는 변명을 들 수 있을 것 같다. 다시금 목표가 생긴 현재 이를 정확히 인지하고 행동으로 실천할 때가 온 것 같다.   인턴 생활을 하기 전까지는 모두 뇌피셜로 코딩을 해오고 공부를 해왔다. 그 때는 BE와 FE의 차이점도 몰랐고 DE가 요즘 핫하다는 이야기만 듣고 무슨 일을 하는지도 모른채로 DE포지션으로 인턴을 지원했었다. 하지만, 현업 생활을 약 4개월정도 하고 난 뒤 내 주변에 현업 생활을 하고 있는 지인들의 이야기가 궁금해졌고 많은 사람들의 이야기를 듣고 싶어 일명 DevTalk을 요청하였다. 그리고 이제는 정확하지는 않지만, 개발자라는 직군이 어떻게 돌아가는지 그리고 내가 어떤 일을 할 수 있고 어떤 일을 하고 싶은지 조금은 알 수 있게 되었다.   우선 나의 문제점을 발견했다. 간단하고 명확하게 말하자면, 나는 현재 ‘코싸개’이다. 생각하지 않고 주어진 문제를 해결하기 위해 수단과 방법을 가리지 않고 그저 코드만을 작성했던 굉장히 질 나쁜 개발자였던 것이다. 생각해보면 내가 이 프로그래밍에 관해 진지하게 공부했던 적이 없다. 학문을 가르치는 대학에서 소프트웨어학과를 진학하고 전공 과목은 집중하지 않고 프로젝트를 어떻게든 이쁘게 만들기 위해서 프로젝트를 위한 공부만 했던 것 같다. 현재 사용하고 있는 기술들은 모두 근본이 되는 알고리즘과 컴퓨터구조론에서부터 비롯된 기술임을 몰랐던 것이다. 나에게 가장 부족한 점은 ‘근본 지식’을 모르는 것이다. 많이 늦었다고 말할 수 있지만, 오늘부터 ‘근본 지식’에 관련하여 몇가지를 체계적으로 공부할 생각이다.   첫번째, 알고리즘이다. 알고리즘을 제대로 배워본 적이 없다. 사실 우리학교의 전공 필수 과목이지만, 너무 어렵고 공부하기도 싫어서 대충 했던 기억이 있다. 그리고 모르는 지식이 있으면 그 때 그 때 구글링으로 일시적으로 지식의 빈자리를 매꿀 뿐이었다. 근본이 되는 알고리즘 서적으로 공부할 계획이다. 두번째, 컴퓨터구조론이다. 컴퓨터구조론 또한 우리학교의 전공 필수 과목이지만, 족보를 바탕으로 공부해서 학점은 좋게 받았지만 남아있는 지식은 없다. 알고리즘과 컴퓨터구조론을 우선 공부하여 인턴 포지션 종료 전 까지 어느정도의 지식을 확보하고 싶은 계획이다.   이후 나는 BE나 DE 직군으로 나아갈 예정이다. 현재 내가 사용하는 주 언어는 python이지만, java에도 관심이 조금씩 가고 있다. 알고리즘과 컴퓨터구조론 공부를 마치고 나면 DE의 근본책과 BE의 근본책 두가지를 구매해서 공부할 예정이다. 이 열정이 언제 식을지 모르겠지만, 현재 내 발등에는 불이 그것도 존나게 뜨거운 불이 떨어졌다.   화이팅이다 홍구야  ","categories": ["doodle"],
        "tags": ["doodle"],
        "url": "/doodle/doodle_01/",
        "teaser": null
      },{
        "title": "test post 입니다.",
        "excerpt":"You’ll find this post in your _posts directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run jekyll serve, which launches a web server and auto-regenerates your site when a file is updated.   Jekyll requires blog post files to be named according to the following format:   YEAR-MONTH-DAY-title.MARKUP   Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and MARKUP is the file extension representing the format used in the file. After that, include the necessary front matter. Take a look at the source for this post to get an idea about how it works.   Jekyll also offers powerful support for code snippets:   def print_hi(name)   puts \"Hi, #{name}\" end print_hi('Tom') #=&gt; prints 'Hi, Tom' to STDOUT.  Check out the Jekyll docs for more info on how to get the most out of Jekyll. File all bugs/feature requests at Jekyll’s GitHub repo. If you have questions, you can ask them on Jekyll Talk.   ","categories": ["test post"],
        "tags": ["test","theme"],
        "url": "/test%20post/test/",
        "teaser": null
      },{
        "title": "2022년 초보 개발자의 취업 계획",
        "excerpt":"   곧 2022년이다. 2021년 한해를 마무리해보자.     2021년 : 마무리   3月-6月 졸업작품    7月-12月 인턴생활    졸업작품 하얗게 불태우고 바로 인턴 생활을 했다. 현업이 되게 궁금했는데 이 부분에 대해서는 굉장히 많은 도움이 된 것 같다. 그리고 Data Engineering 직무도 굉장히 관심 있었는데 직접 경험할 수 있어서 좋았다. 나는 조금 더 다양한 경험을 하고 싶어서  Back End 직무를 공략할 생각이다. 언어는 아무래도 java가 강력하다고 생각해서 java 공부를 시작할 것 같다.    2022년 : 11월 전까지 아래 서류들을 준비해보자      졸업요건 맞추기    토익 또는 오픽    정보처리기사    CS    Algorithm    Java 스킬    ~2022년 포트폴리오 정리    코딩 캠프(네이버부캠,우테코 등등…) 준비      Java Backend Roadmap   ","categories": ["doodle"],
        "tags": ["doodle"],
        "url": "/doodle/doodle_02/",
        "teaser": null
      },{
        "title": "[NLP/Paper]Google's Neural Machine Translation System : Bridging the Gap between Human and Machine Translation 논문 리뷰",
        "excerpt":"Introduction     Neural Machine Translation            End-to-End 학습 접근 방식의 자동 번역       기존 구문 기반 번역의 약점을 극복           Neural Machine Translation 단점            데이터 양과 매개변수가 많아 훈련과 추론 속도가 느림       Rare Word 처리의 문제점       가끔씩 모든 단어에 대해 번역하지 못함           Google’s Neural Machine Translation            LSTM으로 이루어져 있는 8개의 ENCODER와 8개의 DECODER       병렬 처리 개선을 위해 DECODER의 최하층과 ENCODER의 최상층을 ATTENTION으로 연결       번역속도를 높이기 위해 low-precision arithmetic, Rare Word 처리를 위해 WordPiece 사용            Model Architecture       Model Parallelism            모델 병렬화와 데이터 병렬화 모두 사용함       Downpour SGD를 사용하여 데이터 병렬화                    여러개의 모델로 나누어, 여러개의 머신에서 동시에 학습함, 각각 학습된 gradient를 평균내어 모델에 적용           실험에서는 10개의 머신에서 128개의 문장을 Mini-batch로 사용                       모델 병렬화                    머신당 8개의 GPU 사용 ( 각 층 마다 서로 다른 GPU에 할당 )           i번째 레이어의 작업이 종료전에 i+1번째 작업 진행 가능                            Segmentation Approches     Wordpiece Model            띄어쓰기는 _, 단어는 내부단어 통계에 기반하여 띄어쓰기로 분리       띄어쓰기를 _로 치환한 이유는 차후에 문장 복원을 위해       실험에서는 wordpiece를 8K~32K에서 좋은 결과 얻음       wordpiece로도 얻을 수 없었던 rare word는 copy model을 사용                 Mixed Word/Character Model            OOV 처리를 로 하지 않고 문자 단위로 나누어 처리함       시작 문자 , 중간 문자 , 끝 문자        전체 작업 과정에서 유지한 채로 학습한 후 태그를 삭제함             Training Criteria     Maximum-liklihood 학습 방식은 로그 확률 값을 최대화하는 목적 함수 ( BLUE 평가 지표와 부합되지 않음 )        Reward개념의 목적함수 사용        r은 문장 단위 점수 ( 출력 문서와 실제 문서의 차이 계산 )   GLEU 점수 지표 사용 ( 출력 문장과 정답 문장을 1~4 토큰으로 만든 뒤 recall과 precision을 구한 뒤 더 작은 값을 GLEU로 정함 )   ML방식과 RL 방식 혼합하여 사용 이 때, a는 0.017        Quantizable Model And Quantized Inference     NMT은 연산량이 많아 Inference 시간이 오래 걸리는 것이 큰 단점   해결하기 위하여 Quantized inference 수행!        Decoder     Beam Search를 사용하여 점수 함수를 최대화 하는 시퀀스 Y를 찾음   Length normalization            길이가 더 긴 문장의 확률이 떨어지기 때문에 이를 보정하기 위하여 사용       하이퍼 파라미터 a 사용 ( 실험에서는 0.6 ~ 0.7 사용 )           Coverage Panelty            source word xi로 부터 attention weight의 합을 구함       로그를 취했기 때문에 attention weight이 편중되지 않은 source word의 값이 매우 작음 음수를 가지게 됨       실험에서는 a는 0.6 b는 0.2 사용                Experiments And Results     Data set            WMT En -&gt; Fr 36M       WMT En -&gt; De 5M           Evaluation Metrics            BLUE       implicit human evaluation ( BLUE는 번역 점수 잘 못메김 )           Training Procederue            TensorFlow 사용하여 구현       12개의 머신으로 병렬화       [-0.04, 0.04] 사이로 매개변수를 균일하게 초기화       Adam Optimizer와 SGD 혼합하여 사용 ( 첫 60k는 Adam으로 그 후로는 SGD 사용)           Learning Rate는 0.5 ( 1.2M 이후부터 200k 단위마다 반씩 줄여가며 학습 )          Conclusion     Wordpiece 모델은 번역 품질과 inference 속도를 효과적으로 높힘   모델과 데이터의 병렬화는 sequence-to-sequence NMT 모델을 일주일 안으로 효율적으로 훈련시킬 수 있음   Model quantization은 inference 속도를 가속화할 수 있어 대형 모델에 사용하기 용이함   Length-normalization, coverage penalty 등과 같은 추가 세부 사항이 NMT 시스템을 잘 작동시키게 도와줌  ","categories": ["NLP"],
        "tags": ["paper review","nlp"],
        "url": "/nlp/NLP_01/",
        "teaser": null
      },{
        "title": "Docker는 대충 이런 느낌인가?",
        "excerpt":"Docker   내가 느낀 전체적인 도커의 메커니즘 : 어떠어떠한 것을 빌드해서 이미지를 만들고  -&gt; 이미지를 도커로 띄운 뒤 -&gt; 이미지를 받아서 로컬에서 개발작업을 진행   !!많은 시행착오를 거쳐 도커의 메커니즘에 대해 이해가 조금 되었다.     Dockerfile을 만든다.   해당 디렉토리 위치에서 docker build를 한다.   docker build을 하면 이미지가 생성된다.   이미지가 생성됐으면, 해당 이미지로 docker run을 한다.   docker run을 하면 컨테이너가 만들어지고 해당 이미지를 컨테이너 안으로 넣는다?띄운다?   그럼 환경셋팅이 된다…?   틀린 부분도 있겠지만 아주 조오금 조오오오오금 메커니즘이 이해가 됐다.   앗 참고로 음… 저런 run이니 이미지 파일들이니 어떤 컨테이너가 실행중인지를 GUI로 확인할 수 있는 프로그램이 Docker Desktop 같다 ㅎㅎ..   추가적으로 환경세팅을 완료한 것 같다. 음… 위의 6번까지 진행한 후에          컨테이너를 만들면 어떠한 가상환경이 만들어지는 것 같다.            그럼 그 컨테이너에가 파이썬 환경으로 이루어져있고 추가로 다양한 패키지들이 들어있다.            그럼 그 환경을 내가 사용하고 있는 Pycharm과 연동을 하는 거다.       어떻게 하냐면 파이참 프로젝트의 인터프리터를 해당 컨테이너에 있는 파이썬path로 설정해주는거다. *https://i-am-eden.tistory.com/13        그리고 코딩하면 된다 ㅎㅎ ***            도커엔진 - 도커를 실행하면 Dockered라는 데몬 프로그램이 서버로 실행.   **여기서 잠깐! 데몬 프로그램이 뭘까?   https://blogger.pe.kr/770  (포그라운드, 백그라운드, 데몬 프로세스)    https://haruhiism.tistory.com/9            도커실행 : 도커 이미지를 받아서 컨테이너로 실행   ** -it 라는 명령어는 -i와 -t 옵션이 합쳐진 옵션, -i는 호스트와 컨테이너 상호 입출력을 맞추고, -t는 TTY를 활성화해서 컨테이너에 터미널로 입력이 가능하게 한다.   ** TTY가 뭐지?!   https://cosmosproject2015.tistory.com/143 (TTY, PTS, PTY)            도커 volume : 데이터를 컨테이너에 저장하지 않고 호스트에 저장하는 방식   https://www.daleseo.com/docker-volumes-bind-mounts/       도커빌드 : Dockerfile로 사용자 정의 이미지를 만듬   *공부하기 : 도커 아키텍쳐, 컨테이너-OS 간의 통신 구조   ** Docker의 개념 및 핵심 설명 :  https://khj93.tistory.com/entry/Docker-Docker-%EA%B0%9C%EB%85%90    Docker 예제 실습중 갱장히 이상한 오류가 발생했다.   failed to solve with frontend dockerfile.v0: failed to read dockerfile: open /var/lib/docker/tmp/buildkit-mount174403522/Dockerfile: no such file or directory   구글링을 계속 해봤지만 dockerfile -&gt; Dockerfile 로 이름을 바꾸라는 답변밖에 없었다.   하지만, 오류가 고쳐지지 않았고 터미널을 Open한 디렉토리 경로를 상위 폴더 위치로 open을 해서 났던 오류였다 ㅎㅎ  ","categories": ["Docker"],
        "tags": ["Docker"],
        "url": "/docker/docker_01/",
        "teaser": null
      },{
        "title": "TDD와 each map",
        "excerpt":"TDD   TDD(Test Driven Development)란 본격적인 개발에 들어가기 전에 테스트 계획 및 코드를 작성하는 것을 의미한다. 테스트가 개발을 이끌어 나가는 것이다. 예를들어, 개발 중 에러가 발생했을 때 소규모 개발에서는 큰 문제가 되지는 않지만, 대규모의 개발 상황에서는 수 많은 모듈과 함수간 종속성들이 굉장히 많은 시간을 괴롭히게 된다. 이러한 문제점을 해결하기 위해서 테스트 주도 개발이 등장했다.   나는 pytest를 사용할 것이다. https://binux.tistory.com/47   일단, monkeypatch.setattr 살펴보자.  이것은 어떤것을 하냐면, mocking이다. Mocking은 실제 값이 아닌 가짜 값을 만들어내는 것이다.   음 예를들면 Upload 클래스가 있다.   Class Upload    |_ Def Extract   |_ Def Transform   |_ Def Load   이렇게 되어있을 때 나는 Transform 부분만 테스트하고 싶다. 하지만 함수의 종속성으로 인하여 Transform에서 사용되는 data는 Extract로 부터 참조되며 Extract에서 추출되는 data는  특정 라이브러리의 기능을 참조한다. 나는 Transform 부분만 테스트하고 싶지만 이런 경우에 Extract부터 특정 라이브러리으 기능까지 테스트해야되는 상황에 처한 것이다. 이런 경우에 이제 Mocking이라는 기술을 쓴다. pytest에서도 제공하는 function이 있지만, 단순한 예를 하나 들자면 정답과 인풋값을 csv파일이나 등등으로 미리 만들어서 로컬에서 참조하도록 코드를 작성하면 된다.   하지만 이때, 테스트 코드에서 원코드를 실행할 때 원코드의 Extract가 실행 되기 때문에 monkeypatch.setattr 같은 기능으로 해당 function을 사용하지 않고 넘겨주는 기능을 넣어줘야한다.    each map  each map을 알아야한다.  음 지금 내가 하는 것은 DB -&gt; transform -&gt; DB 적재이다. transform에서 전처리 및 parsing을 해주는데, transform에서 이뤄지는 작업은 모든 Dataframe이 메모리 상으로 올라가게 된다. 작은 task면 문제없이 실행 되겠지만, 큰 규모의 task는 메모리를 많이 차지하게 되어 에러가 날 수 있다. 이럴 때 사용 하는 것이 each map이다. each map은 dataframe에서 row 별로 메모리 상으로 올린다. 이후 해당 row에서 특정 처리를 진행 후에 buffer로 옮긴 뒤 DB로 적재를 한다. 이 때 조심해야 하는 부분은 seperate다. row에서 컬럼으로 구분하는 seperate값을 잘 이용해야지 에러가 나지 않을 것이다.    즉! pytest 부분을 더 공부하고 적절한 testset을 생각해보고, testcode를 작성해보자!  ","categories": ["Docker"],
        "tags": ["Docker"],
        "url": "/docker/doodle_03/",
        "teaser": null
      },{
        "title": "[CS:APP] Chapter 01. 컴퓨터 시스템으로의 여행",
        "excerpt":"CS:APP  Computer Systems A Programmer’s Perspective, CSAPP로도 잘 알려져있는 컴퓨터 구조론의 바이블이다. 이 책을 통해서 컴퓨터구조론을 다시 공부해볼 생각이다. 군대 전역 직후 2학년 2학기로 복학해서 컴퓨터 구조론 전공 수업을 들었지만 학점을 위한 공부만 해서 남아있는게 없는 것 같다. 인턴 생활을 하면서 다양한 부분에서 어려움을 겪었는데 기초를 몰라서 헤메고 있다는 느낌을 굉장히 많이 받았다.   첫 페이지를 읽었을 때 이 책으로 정하길 정말 잘했다는 생각이 들었다. 전공 수업때 사용했던 컴퓨터구조론 책은 제작자의 관점에서 기술 되었다는 느낌을 강하게 받았었다. 하지만, CSAPP는 프로그래머의 관점에서 기술하였고, 컴퓨터구조론의 시스템들을 어떻게 사용해서 좋은 프로그램을 개발할 수 있는지를 배울 수 있다고 한다.  인턴 생활을 하면서 실제 현업을 겪고 느꼇던 강한 의문들을 해결할 수 있을 것 같은 느낌이다.   책의 목차는 정보의 표현과 처리로 시작하여 프로그램의 기계어 표현, 프로세서 구조, 프로그램 성능 최적화, 메모리 계층구조, 링커, 예외적인 제어흐름, 가상메모리, 시스템 수준 입출력, 네트워크 프로그래밍, 동시성 프로그래밍  순서로 이어져 있다. 많은 사람들은 6장 메모리 계층구조, 7장 링커의 전까지 읽어도 좋다고 하지만, 가능하다면 12장 동시성 프로그래밍까지 읽어볼 생각이다.   Chapter 01. 컴퓨터 시스템으로의 여행     시스템 구현방식은 변하지만 근본적인 개념들은 변하지 않는다.   프로그래머들로 하여금 컴포넌트들이 어떻게 동작하고 프로그램 성능과 정확성에 어떤 영향을 주는지 알 수 있다.   1.1 정보는 비트와 컨텍스트로 이루어진다.     텍스트 문자 -&gt; 아스키(ASCII) 표준 사용하여 표현 -&gt; 각 문자를 바이트 길이의 정수 값으로 표현 -&gt; 연속된 바이트 파일 저장   1.2 프로그램은 다른 프로그램에 의해 다른 형태로 번역된다.     hello.c 실행 -&gt; 저급 기계어 인스트럭션들로 번역 -&gt; (실행가능 목적 프로그램)으로 합쳐져 바이너리 디스크 파일로 저장  -&gt; 컴파일러 드라이브는 유닉스 시스템에서 소스파일에서 오브젝트 파일로 변경 -&gt; 4개의 단계를 거쳐서 실행            4개의 단계:                    전처리기           컴파일러           어셈블러           링커                           1.3 컴파일 시스템이 어떻게 동작하는지 이해하는 것은 중요하다.     프로그램 성능 최적화하기            eg1) switch문은 if-else문을 연속해서 사용하는 것보다 효율적인가?       eg2) while 루프는 for 루프보다 더 효율적일까?       eg3) 포인터 참조가 배열 인덱스보다 더 효율적인가?           링크 에러 이해하기            eg1) 정적변수와 전역변수의 차이는 무엇인가?       eg2) 다른 파일에 동일한 이름의 두 개의 전역변수를 정의한다면 무슨 일이 일어나는가?           보안 약점 피하기            eg1) 프로그램 스택에 데이터와 제어 정보가 저장되는 방식은 무엇인가?           1.4 프로세서는 메모리에 저장된 인스트럭션을 읽고 해석한다.     인스트럭션이란 : 컴퓨터에게 일을 시키는 단위(기계어)       시스템의 하드웨어 조직            버스 : 시스템 내를 관통하는 전기적 배선군       입출력 장치 : 시스템과 외부세계외의 연결 담당       메인 메모리 : 프로세서가 프로그램을 실행하는 동안 데이터와 프로그램을 모두 저장하는 임시 저장장치       프로세서 : 인스트럭션들을 해독(실행)하는 엔진                    인스트럭션의 요청에 의해 CPU(프로세서)가 실행하는 단순한 작업의 예                            적재(load), 저장(store), 작업(operate), 점프(jump)                                                   프로그램의 실행 (그림 추가 예정 ㅎㅎ..)   1.5 캐시가 중요하다.     hello 프로그램의 기계어 인스트럭션들은 본래 하드디스크에 저장되어 있다.   프로그램이 로딩될 때 이들은 메인 메모리로 복사된다.   이 작업이 시간이 너무 오래 걸려서 “단기간에 필요로 할 가능성이 높은 정보를 임시로 저장하는” 캐시 메모리가 설계 되었다.   캐시 시스템의 이면에 깔려 있는 아이디어는 프로그램이 지엽적인 영역의 코드와 데이터를 액세스하는 경향인 지역성을 활용하였다.   1.6 저장장치들은 계층구조를 이룬다.     모든 컴퓨터 시스템의 저장장치즈들은 메모리 계층구조로 구성되어 있다.   (그림 추가 예정 ㅎㅎ..)   1.7 운영체제는 하드웨어를 관리한다.     운영체제는 두 가지 주요 목적을 가지고 있다.            제멋대로 동작하는 응용프로그램들이 하드웨어를 잘못 사용하는 것을 막기 위해       응용프로그램들이 단순하고 균일한 메커니즘을 사용하여 복잡하고 매우 다른 저수준 하드웨어 장치들을 조작할 수 있도록 하기 위해           위 두가지 목표를 위해 근본적인 추상화를 통해 달성하고 있다.   추상화 결과            프로세스 : 프로세서, 메인 메모리, 입출력장치 모두의 추상화 결과       가상 메모리 : 메인 메모리와 디스크 입출력 장치의 추상화       파일 : 입출력장치의 추상화           1.8 시스템은 네트워크를 사용하여 다른 시스템과 통신한다.     네트워크는 또 다른 입력장치로 볼 수 있다.   시스템이 메인 메모리로부터 네트워크 어댑터로 일련의 바이트를 복사할 때, 데이터는 로컬디스크 드라이브 대신에 네트워크를 통해서 다른 컴퓨터로 이동된다.   1.9 중요한 주제들     Amdahl의 법칙            우리가 어떤 시스템의 한 부분의 성능을 개선할 때, 전체 시스템 성능에 대한 효과는 구 부분이 얼마나 중요한가와 이 부분이 얼마나 빨라졌는가에 관계된다.           동시성과 병렬성            동시성 : 다수의 동시에 벌어지는 일을 갖는 시스템에 관한 일반적인 개념       병렬성 : 동시성을 사용해서 시스템을 보다 더 빠르게 동작하도록 하는 것       쓰레드 수준 동시성                    쓰레드를 이용하면 한 개의 프로세스 내에서 실행되는 다수의 제어흐름을 가질 수 있음                       인스트럭션 수준 병렬성                    프로세서들은 훨씬 낮은 수준에서의 추상화로 여러 개의 인스트럭션을 한 번에 실행할 수 있음                           컴퓨터 시스템에서 추상화의 중요성            추상화의 사용은 전산학에서 가장 중요한 개념!           1.10 요약     컴퓨터 내의 정보는 비트들의 그룹으로 표시   컴파일러와 링커에 의해 바이너리 실행파일들로 번역   프로세서는 메인 메모리에 저장된 바이너리 인스트럭션을 읽고 해석   컴퓨터는 대부분의 시간을 메모리, 입출력장치, CPU 레지스터 간에 데이터를 복사하고 쓰는 데 사용   위와 같은 이유로 시스템의 저장장치들은 계층구조 형성   운영체제 커널은 응용프로그램과 하드웨어 사이에서 중간자 역할 수행   네트워크는 컴퓨터 시스템이 서로 통신할 수 있는 방법 제공   ","categories": ["CS:APP"],
        "tags": ["CS:APP","CS"],
        "url": "/cs:app/csapp_01/",
        "teaser": null
      },{
        "title": "[CS:APP] Chapter 02. 정보의 표현과 처리 (2.1 정보의 저장 ~ 2.2 정수의 표시)",
        "excerpt":"Chapter 02. 정보의 표현과 처리     컴퓨터는 두 개의 값을 갖는 신호로 표현되는 정보를 저장하고 처리한다.   비트 패턴            이진수 체계를 사용하여 여러 비트를 묶어서 양수들을 인코딩하기 위해 사용한다.       표준 문자코드를 사용해서 문서의 글자와 기호를 인코딩할 수 있다.           인코딩            비부호형 : 전통적인 이진수 표시 사용       부호형(2의 보수) : 양수 또는 음수 값을 갖는 부호형 정수를 표시하는 가장 일반적인 방법       부동소수점 : 2진수 버전의 소수 표시방법           이 장을 통해 표시 가능한 숫자의 범위, 비트수준 표시, 산술연산 성질 같은 특성을 도출한다.        2.1 정보의 저장     기계수준의 프로그램은 메모리를 ‘가상메모리’라고 하는 거대한 바이트의 배열로 취급   메모리의 각 바이트는 주소라고 하는 고유한 숫자로 식별할 수 있음   모든 가능한 주소들의 집합을 ‘가상 주소공간’이라고 부름   2.1.1 16진수 표시     1바이트는 8비트로 이루어짐        이진수 표시와 십진수 표시는 비트 패턴을 표시하는 데 매우 불편함 -&gt; 16진수 사용              16진수 &lt;-&gt; 10진수 &lt;-&gt; 2진수 변환을 잘 할 수 있어야 이후 내용이 헷갈리지 않음            2.1.2 데이터의 크기     모든 컴퓨터는 워드 크기(word size)를 규격으로 가지게 됨            w비트 워드 크기를 갖는 컴퓨터에서 가상주소는 0에서 2^w - 1 범위를 가짐       프로그램은 최대 2^w 바이트에 접근 가능                최근에는 32비트 워드 크기를 갖는 컴퓨터에서 64비트 워드 크기를 갖는 컴퓨터로 전환이 보편적임              2.1.3 주소지정과 바이트 순서     여러 바이트에 걸쳐 있는 프로그램 객체들에 있어 두 개의 관습 설정            객체의 주소가 무엇이 되어야 하는지       메모리에 바이트들을 어떻게 정렬해야 하는지           비트 표시 [xw−1, xw−2, . . . , x1, x0]를 갖는 w-비트 정수가 있다.            가장 중요한 바이트 : [xw−1, xw−2, . . . , xw−8]       가장 덜 중요한 바이트 : [x7, x6, . . . , x0]           바이트 저장 방법            리틀 엔디안 : 가장 덜 중요한 바이트가 먼저 오는       빅 엔디안 : 가장 중요한 바이트가 먼저 오는       대부분의 인텔 호환 머신들은 리틀 엔디안 방식으로 동작           바이트 순서가 이슈가 되는 경우            이진 데이터가 네트워크를 통해 다른 컴퓨터로 전송될 때                    리틀 엔디안과 빅 엔디안과의 통신에 있어 바이트 순서가 뒤바뀜                       정수 데이터를 나타내는 바이트들을 살펴볼 때       프로그램이 정상적인 타입 체계를 회피하도록 작성되었을 때           2.1.4 스트링의 표시      그냥 ASCII를 사용한다는 내용,,,    2.1.5 코드의 표현          인스트럭션들의 인코딩이 컴퓨터 타입마다 모두 다름                   컴퓨터 시스템에서 근본 개념은 컴퓨터의 관점에서 볼 때 프로그램이라는 것은 단순히 바이트의 연속이라는 것       2.1.6 부울 Boolean 대수     NOT : ~   AND : &amp;   OR : |        EXCLUSIVE-OR : ^              2.1.7 C에서의 비트수준 연산          C에서는 비트들 간의 부울 연산을 지원                   비트수준 연산은 일반적으로 마스크 연산을 구현할 때 사용한다.             마스크: 비트 연산에 사용되는 데이터            2.1.9 C에서의 쉬프트 연산     비트 패턴을 좌우로 이동시키는 쉬프트 연산 집합 제공   x « k            x는 좌측으로 k비트 이동하고, 중요한 좌측의 k비트가 밀려서 삭제되며, 우측에는 k개의 0으로 채워진다.           x » k            좌측 쉬프트와 비슷하지만 미묘한 차이가 있다.       논리 우측 쉬프트 : 좌측 끝을 k개의 0들로 채움       산술 우측 쉬프트 : 좌측 끝을 가장 중요한 비트를 K개 반복해서 채움           부호형 데이터는 산술 우측 쉬프트, 비부호형 데이터는 논리 우측 쉬프트 사용        2.2 정수의 표시          컴퓨터가 어떻게 정수를 인코딩하고 사용하는지, 아래 그림은 수학적 용어들로 나열한 것              2.2.1 정수형 데이터 타입     서로 다른 크기에 할당된 바이트 수는 컴퓨터의 워드 크기와 컴파일러에 따라 달라진다.   64비트 프로그램에서의 C 정수형 자료형의 일반적인 범위            음수의 범위가 양의 범위보다 1 더 넓은 것에 주목                  2.2.2 비부호형의 인코딩     음수 값을 포함하지 않음        B2U는 Binary 에서 Unsigned의 줄임말               2.2.3 2의 보수 two’s complement 인코딩     음수 값을 포함        B2T는 Binary 에서 Two’s complement의 줄임말                  모든 머신들에서의 호환성을 극대화하는 것을 고려하는 프로그래머라면 자료형들의 보장된 범위를 넘어가는 표현 가능한 값의 특정 범위를 가정해서는 안되며,  부호형 수에 대해서 특정 표시를 가정해서도 안된다.    2.2.4 비부호형과 부호형 간의 변환     C는 서로 다른 숫자 데이터 타입들 간에 캐스팅을 허용한다.   2의 보수에서 비부호형으로의 변환            TMin ≤ x ≤ TMax를 만족하는 x에 대해  x가 0보다 크거나 같으면 x x가 0보다 작으면 x+2^w                  이유는 2의 보수는 가장 중요한 바트 위치에 부호를 표현하는 비트를 사용하는데 Unsigned에서는 부호비트를 사용하지 않기 때문이다.            비부호형에서 2의 보수로의 변환            0 ≤ u ≤ UMax를 만족하는 u에 대해 u가 TMax보다 작거나 같으면 u u가 TMax보다 크면 u-2^w                  이유는 T2U의 이유와 동일하다.  이러한 특성이 산술연산에서 오버플로우를 일으킨다. 2.3장에서 해당 내용이 나올 예정이다.(이해하는데 살짝 오래걸려씀 ㅜㅜ..)            2.2.5 C에서 부호형과 비부호형의 비교     C에서는 부호형과 비부호형 산술연산을 지원한다.            C는 묵시적으로 부호형 인자를 비부호형으로 변환하고, 숫자들이 비음수라고 가정하고 계산을 수행한다.            2.2.6 수의 비트 표시를 확장하기     비부호형 수를 길이가 긴 데이터 타입으로 변환하기 위해서 단순히 앞에 0들을 추가할 수 있다.            영의 확장 zero extension 이라고 알려짐           2의 보수를 보다 긴 데이터 타입으로 변환하려면 앞에 가장 중요한 비트를 복사해서 추가한다.            부호 확장 sign extension 이라고 알려짐                  2.2.7 숫자의 절삭     진짜 진짜 너무 어려웠다… mod가 뭔지 몰랐는데 mod는 나머지 구하기를 뜻하고 있다.       비트의 개수를 줄이는 경우이다.            eg1) int type을 short type으로 캐스팅할 때. 32비트 int를 16비트 short로 절삭한다.           비부호형 수의 절삭            x’ = x mod 2^k 이다.                    삭제되는 모든 비트들은 i ≥ k인 2^i의 자리값을 가진다.           자리값들은 모듈(mod) 계산으로 모두 0이 된다.                           2의 보수 숫자의 절삭            x’ = U2T(x mod 2^k) 이다.                    가장 중요한 비트인 x_k-1이 자리값 2^(k-1) 대신 -2^(k-1)을 갖는 효과를 가진다.                           2.2.8 Signed와 Unsigned에 관한 조언     부호형을 비부호형으로 묵시적인 타입 변환을 하면 다소 직관적이지 않은 동작을 보인다.   비부호형 값들은 워드 길이 데이터를 숫자 값으로 해석하지 않고 단지 비트들의 집합으로 생각하려는 경우에 매우 유용하다.      2.2 정수의 표현 챕터에서 나온 Signed와 Unsigned의 인코딩, 변환, 비교 그리고 비트 확장 및 절삭은 다음 챕터인 산술연산에서 모두 사용된다. 위의 내용들을 정확히 숙지하지 않고 산술연산을 공부하게 되면 많은 어려움을 겪을 수 있다. 이번 챕터를 확실하게 이해하고 다음 챕터로 넘어가자.   ","categories": ["CS:APP"],
        "tags": ["CS:APP","CS"],
        "url": "/cs:app/csapp_02/",
        "teaser": null
      },{
        "title": "[CS:APP] Chapter 02. 정보의 표현과 처리 (2.3 정수의 산술연산)",
        "excerpt":"Chapter 02. 정보의 표현과 처리     2.1 정보의 저장            컴퓨터는 데이터, 인스트럭션등을 어떻게 저장하는지           2.2 정수의 표시            인코딩, 비부호형 및 2의보수                2.3 정수의 산술연산     두 개의 양수를 더해서 음수가 나오는 경우, x&lt;y와 x-y&lt;0이 다른 결과를 보일 때 초보 프로그래머(나포함 ㅋ..)들은 놀라게 된다,            그렇다면 컴퓨터 산술연산의 미묘한 느낌을 이해해보자.               2.3.1 비부호형 덧셈  x,y, 0≤x, y≤e^w가 있다고 하자, 두 합의 계산 범위는 0≤x+y≤2^(w+1)-2를 갖는다. 이 합의 크기를 표현 하기 위해서는 w+1개의 비트가 필요하게 된다.     “워드 크기 증가”는 산술연산의 결과를 완벽하게 표시하려면 필요한 워드 크기를 제한할 수 없다는 것을 의미한다.   프로그래밍 언어들은 고정길이 산술연산을 지원하며 따라서 “덧셈”과 “곱셈”같은 연산은 정수에 대응하는 일반적인 연산들과는 다르다.        x,y에 대해 x+y 정수합을 w비트 길이로 절삭한 후에 다시 비부호형 정수로 나타낸다.                   정상적인 경우에는 x+y값을 유지하지만, 오버플로우의 경우에는 2^w만큼 줄어드는 효과를 낸다.                 산술연산에서 완전한 정수 결과가 그 데이터 타입의 제한된 워드 길이로 나타낼 수 없을 때 이 연산은 “오버플로우한다”고 한다.         2.3.2 2의 보수의 덧셈  2의 보수 덧셈의 결과값이 너무 크거나(양수) 또는 너무 작아서(음수) 표시할 수 없을 때 우리는 어떻게 해야 할지 결정해야 한다.          x+y를 w비트 길이로 절삭한 후에 2의 보수로 결과를 나타내는 연산이다.                    2의 보수의 합이 2^(w-1)을 넘어간다면 양수 오버플로우가 생기고, -2^(w-1) 미만으로 내려간다면 음수 오버플로우가 생겨 각각에 2^w을 더해주거나 빼준다.         2.3.3 2의 보수에서의 비트반전 Negation          비부호형 비트반전                   2의보수 비트반전                   2.3.4 비부호형 곱셈     비부호형 곱셉은 2w비트 정수 곱의 하위 w비트로 주어지는 w비트 값을 만드는 것으로 정의된다.        비부호형 수를 w비트로 절삭하는 것은 해당 값을 2^w로 나눈 나머지를 계싼하는 것과 같다.                  2.3.5 2의 보수 곱셈          2^w로 나눈 나머지를 취하고 비부호형에서 2의 보수형태로 변환하는 것과 같다.                  2.3.6 상수를 사용한 곱셈     정수 곱셉은 매우 느리다. 컴파일러에서 수행되는 중요한 최적화는 상수를 곱하게 되는 경우들을 쉬프트와 덧셈의 조합으로 대체한다.   2의 제곱을 곱하는 경우            k≥0인 모든 k에 대해 x2^k의 w+k 비트수준 표현은 우측에 k개의 0을 추가한 것이다.       eg) 11은 w=4인 경우 [1011]이다. 이것을 왼쪽으로 k=2 쉬프트 하면 [101100]이 되며 이것은 11*4=44를 인코딩한 것이다.           2의 제곱을 곱하면 비부형이건 2의 보수 산술연산이건 오버플로우가 발생할 수 있다.            eg) 위의 예제를 예로 들자면, [101100]을 4비트로 절삭하면 1100을 얻는다.              정수 곱셈이 쉬프트와 덧셈을 사용하는 것보다 훨씬 비용이 많이 드는 연산이기 때문에 C 컴파일러들은 정수가 상수와 곱해지는 경우에 이들을 쉬프트, 덧셈, 뺄셈 등의 조합을 사용해서 제거하려고 노력한다.        2.3.7 2의 제곱으로 나눗셈하기     정수 나눗셈은 정수 곱셉보다 훨씬 느리다. 2의 제곱으로 나누는 것은 오른쪽 쉬프트를 사용한다.   비부호형은 논리 쉬프트, 2의 보수는 산술 쉬프트를 사용한다.   2의 보수 음수에서는 보정값을 더해서 결과값을 0 방향으로 근사하도록 한다.       2.3.8 정수 산술연산에 대한 마지막 고찰  컴퓨터에서 실행되는 “정수” 산술연산은 실제로는 modular 산술연산의 형태로 수행된다. 숫자를 표현하기 위해 유한한 길이의 워드를 사용하기 때문에 가능한 값의 범위가 제한되며 연산의 결과가 오버플로우될 수 있다.  ","categories": ["CS:APP"],
        "tags": ["CS:APP","CS"],
        "url": "/cs:app/csapp_03/",
        "teaser": null
      },{
        "title": "[Java/IntelliJ] IntelliJ 설치 및 Hello world 작성",
        "excerpt":"IntelliJ  학교에서는 java를 배울 때 이클립스(Eclipse)를 사용했다. 하지만 python을 주로 다루면서 파이참(Pycharm)에 익숙해졌다.  java 공부를 하면서 좀 더 친숙한 JetBrains 회사의 개발도구를 사용하려고 한다. 정보를 조금 찾아보니 IntelliJ가 현재 내가 사용하고 있는 환경인 Mac Os에서 이클립스보다 오류가 덜 발생한다고 한다. 통합 개발 환경도 제공한다고 하니 이보다 더 좋을 수가 없다!    해당 포스트는 Mac Os 환경에서 진행되었다.   1. IntelliJ 설치  우선 IntelliJ Mac Os 버전을 설치하려면  IntelliJ 설치 링크 에 접속한다.  각자 Mac 환경에 맞는 파일을 다운로드한다.          java 학습을 위해선 Community Edition으로도 충분하다고 한다. 하지만 본인이 학생 신분이라면 학생 라이센스를 이용한 Ultimate 버전을 추천한다.          아래와 같은 창이 뜨면 설치 성공!      2. Java Project 만들기  위의 창에서 Create New Project를 누르면 아래와 같은 창이 열린다.   왼쪽의 리스트에서는 Java 선택, 오른쪽 리스트에서는 아무것도 선택하지 않고, 오른쪽 아래의 “Next” 버튼을 누른다.          Java 프로젝트를 생성할 때 샘플코드를 생성할지 여부.   본인이 직접 생성할 것이기 때문에 아무것도 선택하지 않고 “Next” 버튼을 누른다.          프로젝트 이름을 정하고 “Next” 버튼을 누른다.      3. Hello World 출력하기  프로젝트 생성이 완료되면, main 메소드가 있는 새로운 Java 클래스를 생성하기 위해 src 폴더를 우클릭한다.  New &gt; Java Class 를 선택한다.          새로 생성할 Java Class 이름을 설정하고 Enter를 친다.          Hello World 출력 코드를 작성한다.  상단의 망치 버튼을 클릭하여 빌드하거나 “control + option + R”키를 눌러 빌드한다.          혹은 out &gt; production &gt; “project_name” &gt; “Class_name” 우클릭 후 Run을 선택해도 된다.          이제 IntelliJ와 함께 Java 공부를 시작해보자,,,!  ","categories": ["Java"],
        "tags": ["Java","IntelliJ"],
        "url": "/java/java_01/",
        "teaser": null
      },{
        "title": "[Java/프로그래머스] Part1 자바 시작하기, Part2 변수와 계산",
        "excerpt":"Part1 자바 시작하기  Java     객체지향언어   C,C++ 문법을 기본으로 개발            C언어에 객체지향 특성 확장           플랫폼에 독립적임            JVM으로 인해서 어떤 플랫폼에서도 실행 가능함           Garbage Collector로 사용되지 않는 메모리 자동적으로 정리해줌   JDK : Java Development Kit        JAVA 개발순서     코드 작성   코드 컴파일   컴파일한 소스를 JVM을 이용하여 실행        주석문     // : 행단위 주석   /* */ : 블럭단위 주석   /** ..*.. */ : 문서화 주석       Part2 변수와 계산   변수  변수 값(Data)을 저장할 수 있는 메모리 공간     java는 강형 언어로써 모든 변수의 타입이 컴파일 시에 결정됨   java 식별자 명명 규칙            첫 번째 글자는 문자 이거나 $,_ 이어야 함       $,_ 이외의 특수문자는 사용 불가능       키워드는 식별자로 사용할 수 없음           java 변수 명명 관례            첫 번째 문자가 소문자인 명사로 정함       여러 단어로 구서된 이름의 경우 두번째 단어부터 첫글자를 대문자로 함 (카멜 표기법)       _를 쓰지 않음               상수  상수란 수식에서 변하지 않는 값을 의미     상수의 선언            final 상수타입 상수명;                    eg) final int J;                           상수 명명 관례            대문자로만 구성된 명사로 정함       여러 단어로 구성된 이릠의 경우 단어 사이에 _을 써서 구분함           상수를 사용해야 하는 경우            값이 변하면 위험한 경우에 상수 사용       값만 봤을 때 무엇을 의미하는지 쉽게 파악할 수 없는 값에도 값 자체를 사용하기 보다는 상수를 사용               기본형 타입  기본형 타입은 가장 기본이 되는 데이터 타입으로써 정수형,실수형,문자형,불린형을 의미한다.     논리형            boolean 1byte 크기, true와 false 중 한 가지 값을 가질 수 있음           문자형            char 2byte 크기, 작은따옴표를 이용하여 한 글자 표현 가능           정수형            int 4byte, long 8byte           실수형            float 4byte, double 9byte           리터럴            CS 분야에서 리터럴이란, 소스 코드의 고정된 값을 대표하는 용어       리터럴은 일종의 값이다. true, false, 10, 11.1, ‘a’등 이런 값 자체를 리터럴이라고 함           사용 방법            long : 값을 적을 때 뒤에 l이나 L 적어야함       float : 값을 적을 때 뒤에 f나 F를 적어야함               기본형 타입변환     묵시적 형변환            크기가 작은 타입을 크기가 더 큰 타입으로 바꿀 때에는 묵시적으로 형을 바꾸어 줌           명시적 형변환            크기가 더 큰 타입을 작은 타입으로 바꿀 때에는 명시적으로 변환 해주어야 함       eg) long x = 20; int y = (int) x;               연산자 우선순위     최우선연산자 ( ., [], () )   단항연산자 ( ++,–,!,~,+/-   : 부정, bit변환&gt;부호&gt;증감)            단, 후위연산자(a++)은 우선순위가 낮음       eg) a=5, x = a++ - 5 라면 x는 0, a는6           산술연산자 ( *,/,%,+,-,shift) &lt; 시프트연산자 ( »,«,»&gt; ) &gt;   비교연산자 ( &gt;,&lt;,&gt;=,&lt;=,==,!= )   비트연산자 ( &amp;,|,,~ )   논리연산자 (&amp;&amp; , || , !)   삼항연산자 (조건식) ? :   대입연산자 =,*=,/=,%=,+=,-=  ","categories": ["Java"],
        "tags": ["Java","프로그래머스 자바 입문"],
        "url": "/java/java_02/",
        "teaser": null
      },{
        "title": "[Java/프로그래머스] Part3 제어문",
        "excerpt":"Part3 제어문   if 조건문  조건식의 연산 결과에 따라 블록 내부 문장의 실해 여부를 결정할 수 있다.     if 문   if - else 문   if - else if - else 문       논리 연산자  논리 연산자의 피연산자는 블린 타입만 사용할 수 있다. 결과는 불린값이다.                                     OR :                                       AND : &amp;&amp;   NOT : !   EXCLUSIVE-Or : ^       삼항 연산자     조건식 ? 피연산자1 : 피연산자2            조건식의 결과가 true 라면 결과는 피연산자1       조건식의 결과가 false 라면 결과는 피연산자2               switch문  switch문은 어떤 변수의 값에 따라서 문장을 실행할 수 있도록 하는 제어문이다.   switch(변수){         case 값1 :              실행문;              break;         case 값2 :              실행문;              break;           default;         }     break를 쓰지 않으면 value값이 값1일 경우 값1, 값2, default를 모두 실행한다.   JDK7 이후에는 문자열 타입의 변수도 가능함      당연한걸 수도 있지만, case 에 조건문을 달면 오류가 난다 하하,,,        while문  조건문의 실행 결과가 true일 동안 반복해서 실행한다.   while(조건문){         실행문;      }      do while문  while 문의 경우 조건이 만족되지 않을 때 실행되지 않지만, do while 문은 무조건 한번은 수행 된다.      do{         실행문;     }while(조건문);     실행문을 한번은 실행하고 싶을 경우에 사용됨       for 반복문  for 구문 자체에 변수 초기화, 조건식, 증감식이 한줄로 표현됨       for(초기화식; 조건식; 증감식){         실행문;         실행문;     }  ","categories": ["Java"],
        "tags": ["Java","프로그래머스 자바 입문"],
        "url": "/java/java_03/",
        "teaser": null
      },{
        "title": "[Java/프로그래머스] Part4 배열",
        "excerpt":"Part4 배열   배열 만들기  배열은 같은 데이터 타입을 가진 연속된 메모리 공간으로 이루어진 자료구조이다.   int[] array1 = new int[4]; int[] array2 = new int[]{1,2,3,4}; int[] array3 = {1,2,3,4};      배열 사용하기     배열에 접근할 때에는 인덱스를 통해서 접근한다.   배열의 길이를 알아내는 방법 : array.length        2차원 배열  2차원 배열이란 배열의 배열이다.   int[][] array4 = new int[3][4];  int[][] array5 = new int[3][]; //위와 같이 선언하면 array5는 3개짜리 배열을 참조한다. 3개짜리 배열은 아직 참조하는 배열이 없다는 것을 의미. array5[0] = new int[1]; //정수를 하나 담을 수 있는 배열을 생성해서 array5 의 0 번째 인덱스가 참조한다.   array5[1] = new int[2]; //정수를 두개 담을 수 있는 배열을 생성해서 array5 의 1 번째 인덱스가 참조한다.   array5[2] = new int[3]; //정수를 세개 담을 수 있는 배열을 생성해서 array5 의 2 번째 인덱스가 참조한다.       for each  for문 안에 (value:array), array의 값 하나씩 value로 매칭된다.     python for in 이랑 똑같은거 같아서 너무 반갑다!    int[] iarr = {10,20,30,40,50};  for(int value:iarr){     System.out.println(value); } ","categories": ["Java"],
        "tags": ["Java","프로그래머스 자바 입문"],
        "url": "/java/java_04/",
        "teaser": null
      },{
        "title": "[CS:APP] Chapter 02. 정보의 표현과 처리 (2.4 부동소수점)",
        "excerpt":"Chapter 02. 정보의 표현과 처리     2.1 정보의 저장            컴퓨터는 데이터, 인스트럭션등을 어떻게 저장하는지           2.2 정수의 표시            인코딩, 비부호형 및 2의보수           2.3 정수의 산술연산            비부호형 및 2의보수의 덧셈,뺄셈,곱셉,나눗셈                2.4 부동소수점     부동소수점 표현은 V = x*2^y 형태의 소수를 인코딩한다.   거의 모든 컴퓨터가 IEEE 부동소수점이라고 알려지게 된 방식을 지원한다.        2.4.1 비율이진수(Fractional Binary Numbers)     부동소수점에 대해서 이해하기 위한 첫 단계는 비율 이진수에 대해 생각해보는 것이다.            숫자들의 자리값은 십진 소수점 부호(‘.’)에 상대적으로 정의된다.                  위와 같은 방식으로 binary 표기법으로 생각해보자.            부호 ‘.’는 이진 소수점이 되고, 좌측의 브트들은 비음수의 2의 제곱을 자리값으로 가지며, 우측은 2의 음의 제곱을 자리값으로 갖는다.       이진 소수점을 한 자리 우측으로 이동하면 2로 곱한 효과, 좌측으로 이동하면 2로 나눈 효과를 가진다.                  이진수 표기는 x*2^y로 나타낼 수 있는 수만 표시할 수 있다.   이진 표시를 길게 늘려서 정확도를 높이도록 근사해야 한다.        2.4.2 IEEE 부동소수점 표시     IEEE 부동소수점 표준은 수를 V = (-1)^sM2^E 형태로 나타낸다            s는 음수와 양수를 결정한다.       유효숫자 M은 비율 이진수다.       지수 E는 2의 제곱으로 자리값을 제공한다.           부동소수점 수의 비트 표시는 이 값들을 인코딩하기 위해 세 개의 필드로 나누어진다.            한 개의 부호 비트 s는 부호 s를 직접 인코딩한다.       k비트 지수 필드 exp = ek-1…e1e0는 지수 E를 인코딩한다.       n비트 비율 필드 frac = fn-1 … f1f0는 유효숫자 M을 인코딩한다.                 Case 1: 정규화 값 Normalized Values            가장 일반적인 경우       exp의 비트 패턴이 모두 0은 아니며, 모두 1이 아니어야 한다.       E = e - Bias (Bias = 2^(k-1) - 1)       비율 필드 frac은 비율 값 f       유효 숫자 M = 1 + f 로 정의           Case 2: 비정규화 값 Denormalized Values            지수 필드가 모두 0일 때 나타낸 수는 비정규화 형태를 갖는다.       E = 1 - Bias       M = f = 0       비정규화 숫자들의 기능은 0.0에 매우 가까운 값들을 나타냄                    이들은 점증적 언더플로우라고 알려진 특성을 제공 (가능한 숫자 값들이 0.0 근처에서 같은 간격을 갖는다는 의미)                           Case 3: 특수 값 Special Values            지수 필드가 모두 1인 경우                    비율 필드가 모두 0이면, 결과값은 무한대를 나타냄           비율 필드가 0이 아니면 NaN(not a numbuer)                                   2.4.3 숫자 예제      2.4.4 근사법 Rounding     부동소수점 산술연산은 표시방법이 제한된 범위와 정밀도를 갖기 때문에 실제 연산의 근사값을 사용할 수밖에 없다.   “가장 유사한” 값 x를 체계적으로 계산하는 방법을 근사rounding 연산이다.   네가지 근사 모드를 정의함            짝수근사법(round-to-even): 가장 가까운 값, 중간에 위치할 경우 짝수를 향해 근사함       영방향근사 모드(round toward-zero): 양수 값을 아래쪽으로, 음수를 위쪽으로 근사함       하향근사 모드(round-down): 양수와 음수를 모두 아래쪽으로 근사함       상향근사 모드(round-up): 양수와 음수를 모두 위쪽으로 근사함                2.4.5 부동소수점 연산     부동소수점 값 x,y를 실수로 보고, 일부 연산이 실수들에 대해 정의된다면 Round(x,y)가 되는데, 이것은 실수 연산의 정확한 결과 값을 근사한 것이다.   부동소수점 덧셈에서 결합법칙이 성립하지 않는 것은 그룹의 특징 중에서 빠진 가장 중요한 부분이다.   교환법칙은 성립하지만, 결합벅칙은 성립 되지 않는다.        C에서 부동소수점     C는 짝수 근사모드를 사용한다.   int에서 float로, 숫자는 오버플로우할 수 없지만, 근사될 수 있다.   int나 float에서 double로, 정확한 수치 값은 보존될 수 있다.   double에서 float로 범위가 더 작아지기 때문에 값이 오버플로우하여 무한대가 될 수 있따.   float나 double에서 int로, 값은 0 방향으로 근사된다        2.5 요약     정보를 비트로 인코딩하며, 이들은 일반적으로 연속된 바이트들로 구성된다.   대부분의 머신들은 정수를 인코딩하기 위해 2의 보수를 사용하고 부동소수점을 인코딩하기 위해 IEEE 표준 754를 사용한다.   부호형 및 비부호형 정수를 캐스팅할 때 비트 패턴을 유지하려고 하는데, T2U, U2T 함수들에 의해 나타난다.   제한된 길이는 숫자들이 나타낼 수 있는 범위를 넘어설 때 오버플로우를 발생시킨다.   비부호형과 2의 보수 산술연산은 결합법칙, 교환법칙, 분배법칙 등의 정수 산술연산의 많은 특성을 만족한다.   결합법칙, 교환법칙, 분배법칙 특성을 쉬프트와 2의 제곱의 곱셈 간 관계와 함께 사용된다.   부동소수점 산술연산은 매우 조심스럽게 사용해야한다.            제한된 범위와 정밀도를 갖기 때문이며, 결합법칙 같은 일반적인 수학 법칙을 따르지 않기 때문이다.                   부동소수점을 마지막으로 Chapter2 정보의 표현과 처리를 마무리하였다. 데이터 엔지니어링 인턴 포지션에서 ETL 작업을 하며 소수점 데이터에 접근 했을 때  R코드와 Python의 소수 처리 방식이 달라서 해당 오류를 잡는데 많은 시간이 걸린 경험이 있었다. 그 때 당시에는 대충 방식이 다르구나~ 로 끝났지만, 이번 챕터를 공부하게 되면서 아 부동소수점의 표현 방식 때문에(책은 C를 기준으로 하고 있지만,,,) 그런 오류가 잡혔던 거구나 하고 시야가 조금 더 넓어진 것 같다.  2.2절의 캐스팅과 2.4절의 부동소수점을 공부하면서 꽤 많은 애를 먹었지만, 앞으로 코딩을 하면서 캐스팅이나 소수점을 다룰 때 더 주의 깊게 다룰 수 있을 것 같다.   ","categories": ["CS:APP"],
        "tags": ["CS:APP","CS"],
        "url": "/cs:app/csapp_04/",
        "teaser": null
      },{
        "title": "[NLP/Paper]MASS:Masked Sequence to Sequence pre-training for Language Generation 논문 리뷰",
        "excerpt":"Introduction     Pre-training은 최근 NLP 분야에서 활발한 연구가 이루어지는 주제 중 하나   Google의 Bert와 XLNet은 자연어 이해(NLU) Task에서 큰 성공을 거둔 모델   NLU Task 외에 Sequence to Sequence를 기반으로 하는 자연어 생성 Task가 있음            Machine Translation, Abstract Text Summarization           자연어 생성 Task는 Encoder - Attention - Decoder 구조의 모델들이 사용됨   Encoder-Attention-Decoder       Encoder는 Source sequence X를 input으로 받은 후, 이를 hidden representations sequence로 변형   Decoder는 Encoder로부터 hidden representations sequence를 전달 받아 Target Sequence Y를 생성하며, 이 때 Attention mechanism이 학습한 Input에 대한 정보를 함께 참조       Related Works       Bert와 XLNet은 Language Understanding을 위해 Encoder를 Pre-train   GPT는 Language Modeling을 위해 Decoder를 Pre-train   이전 모델들은 Encoder, Attention, Decoder를 함께 Pre-train하지 못했음       MASS:Masked Sequence to Sequence Pre-training       새로운 모델 MASS는 Masked Sequence to Sequence Pre-training의 약자   Input에서 K개의 토큰 Fragment를 임의로 지정해 Masking   마스킹 된 Fragment를 Encoder-Attention-Decoder를 거쳐 예측하도록 훈련시킴   Encoder에서 Masking되지 않은 토큰들이 Decoder에서 Masking됨에 따라, Decoder는 Encoder가 제공한 hidden representation과 Attention 정보만을 참고하여 Masking된 토큰들을 예측해야함, 이를 통해 Encoder-Attention-Decoder가 함께 Pre-train될 수 있는 환경을 제공   Encoder는 Encoder에서 Masking되지 않은 토큰들의 정보를 잘 추출할 수 있도록 학습, 이를 통해 Language Understanding 능력 개선   Decoder는 Encoder에서 Maksing된 토큰들에 대한 예측을 연속적으로 수행해야 하기 때문에 Language Modeling 능력을 학습하게 됨        K는 Encoder에서 Masking되는 토큰 개수를 결정하는 하이퍼 파라미터   하이퍼 파라미터 K를 조정함으로써 MASS는 BERT의 Masked LM과 GPT의 Standard LM을 모두 구현 가능   BERT Masked LM            K=1일 때, Encoder에서는 하나의 토큰이 Masking되고, Decoder는 Masking된 하나의 토큰을 예측해야 하므로 BERT의 Masked LM과 같아짐           GPT Standard LM            하이퍼 파라미터 K가 Input 문장의 전체 길이인 m과 같을 때 Encoder의 모든 토큰들이 Masking됨       Decoder는 Encoder로부터 어떠한 정보도 부여 받지 않은 채 모든 토큰을 예측해야 하므로, GPT의 Standard LM과 같아짐                m - Input sequence의 전체길이   u - Maskinge된 Fragment의 시작점   v - Masking된 Fragment의 끝점   X^u - u부터 v까지의 fragment   X^\\u:v - u부터 v까지 Making된 Input Sequence       Experiments     Model Configuration            1024 embeding/hidden size, 4096 feed-forward filter size를 가진 6-layer encoder, 6-layer decoder로 구성된 기본 모델 구조 Transformer           Datasets            2007 - 2017 WMT News Crawl datasets 190M English, 62M French, German 270M       MASS의 효과 검증을 위한 low-resource language Romanian           Pre-training Details            Fragment length K의 길이를 문장에 있는 총 토큰 수의 약 50%로 설정하고 정확성의 변화를 비교하기 위하여 K를 바꿔가며 실험               Experiment:Unsupervised Machine Translation       Unsupervised Machine Translation Task에 있어 이전 모델들, 최근까지 SOTA였던 Facebook의 XLM모델과 MASS의 성능 비교를 수행   Facebook의 XLM은 Encoder와 Decoder를 각각 Bert의 Masked LM과 standard LM으로 Pre-train 시킨 모델   MASS는 6개의 Machine Translation Task에 있어 XLM을 능가하는 성능, SOTA 기록        Experiment:Low-resource Translation       Low-resource Machine Translation : bilingual 트레이닝 데이터셋이 부족한 환경에서의 기계번역   Englisgh-Fench, English-German, English-Romanian 데이터 셋을 10K, 100K, 1M의 사이즈로 늘려가며 Low-resource 환경에서의 Machine Translation 테스트   MASS는 모든 스케일에서 Low-resource MT의 baseline 성능을 능가   데이터셋의 사이즈가 작을수록 더욱 두드러지게 나타남        Experiment:Abstractive Summarization       Pre-trained Bert를 인코더로 Pre-trained Language Model을 Decoder로 사용한 BERT+LM 모델과 DAE(Denoising Auto-Encoder), MASS의 Abstractive Summarization 성능을 Gigaword Corpus에 대해 비교   MASS는 BERT+LM과 DAE 두 모델의 Abstractive Summarization 성능을 모두 능가        Experiment:Conversational Response Generation       Abstractive Summarization 성능 비교에 사용되었던 BERT+LM 모델과 Baseline, MASS의 Conversational Response Generation 성능을 Cornell Movie Dialog Corpus에 대해 비교   MASS가 BERT+LM 모델과 Baseline 보다 낮은 Perplexity를 기록하며 더 좋은 성능을 보여줌        The probability formulation       (a) 영어-프랑스 번역의 영어, (b) 영어-프랑스 번역의 프랑스어, (c) 영어-프랑스 번역의 BLUE score (d) Text Summarization의 ROUGE score, (e) conversational response generation의 PPL   하이퍼 파라미터 K를 다르게 설정해가며, MASS의 성능에 대한 다양한 실험 결과   실험을 통해 K가 문장의 절반 정도 크기에 해당할 때, downstream task에서 가장 좋은 성능을 보임을 알게 됨            문장 내 절반의 토큰을 Masking하는 것이 Encoder와 Decoder의 Pre-training에 있어 적절한 균형을 제공           K가 1(BERT), K가 m(GPT) 이었을 때는 downstream task에서 좋은 성능이 나오지 않음 이 이유는 MASS가 Sequence to Sequence 기반의 Language Generation Task에 이점을 지니고 있음을 반증       Couclusion     MASS는 Sequence to Sequence 기반의 다양한 Language generation Task에 있어 좋은 성능 기록   Language generation에서 좋은 성능을 기록한 MASS가 BERT와 XLnet이 뛰어난 결과를 기록한 Natural Language Understanding Task에서도 좋은 성능을 보일 것인지 차후 실험해볼 예정   또한 MASS 모델이 Image와 Video와 같은 NLP와 다른 도메인에서도 Sequence to Sequence 기반의 생성 Task를 수행해낼 수 있을지에 대한 추가 실험도 해볼 예정  ","categories": ["NLP"],
        "tags": ["paper review","nlp"],
        "url": "/nlp/NLP_02/",
        "teaser": null
      },{
        "title": "[NLP/Paper]ELMO - Deep contextualized word representations 논문 리뷰",
        "excerpt":"Introduction     Pre-trained word representations are a key component in many neural language understanding models.   Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence.   We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus.   In many experiments, the ELMo representation has been shown to be very excellent, and the error rate is relatively reduced by 20%       Related Works     Pretrained word vector의 활용이 표준화 되었지만, 하나의 단어에 하나의 벡터를 부여하다보니 context-independent한 문제가 있었다.   워드 임베딩을 풍부하게 하기 위해, subword information을 활용하거나 다의어의 경우 의미별로 다른 벡터를 학습시키는 방법이 등장하였다.            context2vec       CoVe           이전 연구에 의하면 biRNN의 서로 다른 레이어가 다른 형태의 정보를 인코딩하는데, 본 연구에서도 유사한 효과가 나타났다.       Model     ELMO            ELMO word representations are functions of the entire input sentence.       They are computed on top of two-layer biLMs with character convolutions, as a linear function of the internal network states.       This setup allows us to do semi-supervised learning, where the biLM is pre-trained at a large scale and easily incorporated into a wide range of existing neural NLP architectures.                 Pretrained BiLM          Task-specific ELMo Embedding         Using biLMs for supervised NLP tasks            기존의 임베딩 벡터와 함께 사용된다.       ELMo 표현을 만드는데 사용된 사전 훈련된 언어 모델의 가중치는 고정시키고, 각 층의 가중치와 스칼라 파라미터는 훈련 과정에서 학습된다.                  Evaluation         6개의 NLP task에서 에러율을 6~20% 줄였다.   6개의 NLP task에서 높은 점수를 기록했다.        Analysis        기존에 top layer output만 사용 한 것 대비 성능 향상을 검증했다.   대부분의 경우 Regularization parameter λ 가 작을수록 성능이 더 좋아지는 경향이 있다.          일부 task(SNLI, SQuAD) 에서는 ELMo 벡터를 Output에 다시 concat 시키는 것이 효과가 있다.          GloVe 단어 벡터에서 ‘play’와 비슷한 단어는 품사를 변형한 것 또는 스포츠에 관한 유사 단어만 뜬다.   biLM에서는 문맥을 고려한다는 것을 알 수 있다.          biLM의 첫 번째 레이어는 syntactic 정보를, 두 번째 레이어는 semantic 정보를 더 잘 인코딩 하는 것으로 나타난다.   이는 biLM의 모든 레이어를 사용하는 것이 성능향상에 도움이 된다는 것을 증명한다.          ELMo를 활용하면 같은 성능을 내는 데에 있어 훨씬 학습이 효율적임을 알 수 있다.       Conclusion     We have introduced a general approach for learning high-quality deep context-dependent representations from biLMs, and shown large improvements when applying ELMo to a broad range of NLP tasks.   Through ablations and other controlled experiments, we have also confirmed that the biLM layers efficiently encode different types of syntactic and semantic information about words in-context, and that using all layers improves overall task performance.  ","categories": ["NLP"],
        "tags": ["paper review","nlp"],
        "url": "/nlp/NLP_03/",
        "teaser": null
      },{
        "title": "[CS:APP] Chapter 03. 프로그램의 기계수준 표현 (3.1 역사점 관점 ~ 3.4 정보 접근하기)",
        "excerpt":"Chapter 03. 프로그램의 기계수준 표현     컴퓨터는 데이터를 처리하고, 메모리를 관리하고, 저장장치에 데이터를 읽거나 쓰고, 네트워크를 통해 통신하는 등의 하위 동작들을 인코딩한 연속되 바이트인 기계어 코드를 실행한다.   컴파일러는 프로그램 언어의 규칙, 대상 컴퓨터의 인스트럭션 집합, 운영체제의 관례 등에 따라 기계어 코드를 생성한다.   어셈블러 코드로 프로그램을 짤 때는 프로그래머가 계산을 하기 위해 사용해야 하는 저급 인스트럭션들을 명시해야 한다.   기계어 코드를 배우면 컴파일러의 최적화 성능을 알 수 있으며, 코드에 내제된 비효율성을 분석할 수 있다.   이 장은 X86-64에 기초하고 있다.        3.1 역사적 관점     x86이라고 통칭하는 인텔 프로세서 제품군은 오랜 기간 진화를 통한 개발을 해왔다.            대충 엄청 많은 프로세서들이 나열 되는데, 팬티엄4E(2004, 125M 트랜지스터) 하이퍼쓰레딩 기법의 추가와 AMD사에서 개발한 IA32의 64비트 확장 구현인 EM64T가 추가된 해당 모델을 x86-64라고 부른다고 한다.                 3.2 프로그램의 인코딩     C 언어에서 gcc 명령은 소스 코드(test.c)를 실행 코드로 변환하기 위해 일련의 프로그램들을 호출한다.            C 전처리가 #include로 명시된 파일을 코드에 삽입해 주고 #define으로 선언된 매크로를 확장해준다.       컴파일러는 소스파일의 어셈블러 버전(test.s)를 생성한다.       어셈블러는 어셈블리 코드를 바이너리 목적코드인(test.o)로 변환한다.                    목적코드는 기계어 코드의 한 유형이다. - 모든 인스트럭션과 바이너리 표현을 포함하고 있지만 전역 값들의 주소는 아직 채워지지 않았다.                       마지막으로 링커가 목적코드 파일을 라이브러리 함수들을 구현한 코드와 함께 합쳐서 최종 실행 파일인 p를 생성한다.           커맨드 라인 옵션으로 -0g를 주면 C 코드의 전체 구조를 따르는 기계어 코드를 생성하는 최적호 수준을 적용한다.            높은 수준의 최적화를 적용하면 만들어진 코드가 너무 많이 변경되어 본래의 코드와 생성된 기계어 코드 간의 관계를 이해하기 어렵다.                3.2.1 기계수준 코드     컴퓨터 시스템은 보다 간단한 추상화 모델을 이용해서 세부 구현내용을 감추면서 추상화의 여러 가지 다른 형태를 사용하고 있다.            기계수준 프로그램의 형식과 동작은 인스트럭션 집합구조 즉 “ISA”에 의해 정의된다.                    프로세서의 상태, 인스트럭션의 형식, 프로세서 상태에 대한 각 인스트럭션들의 영향들을 정의한다.                       기계수준 프로그램이 사용하는 주소는 가상주소이며, 메모리가 매우 큰 바이트 배열인 것처럼 보이게 하는 메모리 모델을 제공한다.           컴파일러는 추상화된 실행모델로 표현된 프로그램을 프로세서가 실행하는 매우 기초적인 인스트럭션들로 변환하는 대부분의 일을 수행한다.        3.2.2 코드 예제       기계어 코드의 몇몇 특징과 이들의 역어셈블된 표현에 주목할 필요가 있다.            x86-64 인스트럭션들은 1에서 15바이트 길이를 갖는다.       인스트럭션의 형식은 주어진 시작 위치에서부터 바이트들을 기계어 인스트럭션으로 유일하게 디코딩할 수 있도록 설계한다.       역어셈블러는 기계어 코드 파일의 바이트 순서에만 전적으로 의존한다.                3.3 데이터의 형식     인텔 프로세서들이 근본적으로 16비트 구조를 사용하다가 추후에 32비트로 확장했기 때문에 인텔은 “워드”라는 단어를 16비트 데이터 타입을 말할 때 사용한다.           3.4 정보 접근하기     x86-64 주처리장치 CPU는 64비트 값을 저장할 수 있는 16개의 범용 레지스터를 보유하고 있다.            이들 레지스터는 정수 데이터와 포인터를 저장하는데 사용한다.                   3.4.1 오퍼랜드 식별자 specifier     대부분의 인스트럭션은 하나 이상의 오퍼랜드를 가진다.   오퍼랜드는 연산을 수행할 소스 값과 그 결과를 저장할 목적지의 위치를 명시한다.   소스 값은 상수로 주어지거나 레지스터나 메모리로부터 읽을 수 있다. 결과 값은 레지스터나 메모리에 저장된다.   세가지 타입으로 나뉘어짐            immediate로, 상수값을 말한다.       register는 레지스터의 내용을 나타내며       Memory, 메모리 참조로 유효주소라고 부르는 계산된 주소에 의해 메모리 위치에 접근하게 된다.                   3.4.2 데이터 이동 인스트럭션     가장 많이 사용되는 인스트럭션은 데이터를 한 위치에서 다른 위치로 복사하는 명령어다.            MOV 클래스 : 소스 위치에서 데이터를 목적지 위치로 어떤 변환도 하지 않고 복사한다.           소스 오퍼랜드는 상수, 레지스터 저장 값, 메모리 저장 값을 표시한다.   목적 오퍼랜드는 레지스터 또는 메모리 주소의 위치를 지정한다.   x86-64는 데이터 이동 인스트럭션에서 두 개의 오퍼랜드 모두가 메모리 위치에 올 수 없도록 제한하고 있다.              3.4.3 코드 예제           C언어에서 “포인터”라고 부르는 것이 어셈블리어에서는 단순히 주소를 나타낸다.     포인터를 역참조하는 것은 포인터를 레지스터에 복사하고, 이 레지스터를 메모리 참조에 사용하는 과정으로 이루어진다.     지역변수들은 메모리에 저장되기보다는 종종 레지스터에 저장된다.           3.4.4 스택 데이터의 저장과 추출 push,pop     push와 pop은 프로그램 스택에 데이터를 저장(push)하거나 스택에서 데이터를 추출(pop)하기 위해 사용한다.   스택은 프로시저 호출을 처리하는 데 중요한 역할을 한다.   프로그램 스택은 메모리의 특정 영역에 위치한다.   스택의 탑top 원소가 모든 스택 원소 중에서 가장 낮은 주소를 갖는 형태다.   스택은 pop이 되어도 stack의 top을 표현하는 주소가 올라간거지 값은 여전히 pop이 된 위치에 남아있다.   스택이 프로그램 코드와 다른 형태의 프로그램 데이터와 동일한 메모리에 저장되기 때문에 프로그램들은 표준 메모리 주소지정 방법을 사용해서 스택 내 임의의 위치에 접근할 수 있다.      ","categories": ["CS:APP"],
        "tags": ["CS:APP","CS"],
        "url": "/cs:app/csapp_05/",
        "teaser": null
      },{
        "title": "[CS:APP] Chapter 03. 프로그램의 기계수준 표현 (3.5 산술연산과 논리연산 ~ 3.7 프로시저)",
        "excerpt":"Chapter 03. 프로그램의 기계수준 표현   3.5 산술연산과 논리연산   3.6 제어문   3.7 프로시저  ","categories": ["CS:APP"],
        "tags": ["CS:APP","CS"],
        "url": "/cs:app/csapp_06/",
        "teaser": null
      },{
        "title": "[NLP/Paper]Multi-Task Deep Neural Networks for Natural Language Understanding 논문 리뷰",
        "excerpt":"Introduction     Multi-Task Learning(MTL)은 새로운 Task를 학습하는 데 도움이 되도록 이전 작업에서 학습된 지식을 적용하는 인간 학습 활동에서 영감을 받음   Deep neural networks(DNN)를 이용한 representation learning에 MTL을 적용하는 것에 대한 관심이 높아지고 있음            DNN을 이용한 representation learning은 많은 양의 데이터를 요구함, MTL은 많은 task에서의 supervised labeled data를 제공함       MTL은 특정 Task에 Overfitting 되지 않도록 Regularization 효과를 줌           MTL과 대조적으로, Language Model은 대용량의 unsupervised dataset을 활용하여 모델을 학습함            ELMo, GPT, BERT           MT-DNN은 Language Model Pre-Training을 활용한 BERT에 Multi-task learning을 적용하여 성능을 개선한 모델      Tasks     GLUE의 9개 task를 MTL에 활용   Single Sentence Classification            하나의 문장이 주어졌을 때 문장의 Class를 분류하는 Task       CoLA : 문장이 문법적으로 맞는지 분류 (True/False)       SST-2 : 영화 Review 문장의 감정 분류 (Poistive/Negative)           Text Similarity            문장 쌍이 주어졌을 때, 점수를 예측하는 Regression Task       STB-B : 문장 간의 의미적 유사도를 점수로 예측           Pairwise Text Classification            문장 쌍이 주어졌을 때, 문장의 관계를 분류하는 Task       RTE, MNLI : 문장 간의 의미적 관계를 3가지로 분류 (Entailment, Contradiction, Neutral) – QQP, MRPC : 문장 간 의미가 같음 여부를 분류 (True/False) – Relevance Ranking – QNLI : 질문과 해당 지문 중 한 문장이 쌍으로 주어졌을 때 해당 지문 문장에 질문의 답이 있는지 여부를 분류 (True/False) – MT-DNN에서는 이를 Rank 방식으로 바꾸어 모든 지문 문장에 정답이 있을 가능성을 Scoring 하여 가장 Score가 높은 지문 문장을 True로 분류하는 방식으로 Task 수행              Model Architecture         Lexicon Encoder            Token Embedding                    맨 앞에 [CLS] 토큰을 추가. 추후 Output에서 Classification 등을 위해 사용됨           만약 문장쌍이 들어온다면 각 문장은 Wordpiece로 Toenization 되며 [SEP] Token이 두 문장 사이의 구분자로 사용됨                       Sentence Embedding - 1번째 혹은 2번째 문장임을 표현하는 Vector       Positional Embedding - 각 Token의 위치 정보를 표현하는 Vector                 Transformer Encoder            Lexicon Encoder로 부터 각 Token의 Input Vector를 입력으로 받아 Ouput Vector 추출       BERT 모델과 달리 task별로 fine-tunning하지 않고 MTL로 fine-tunning 함                 Single-Sentence Classification Ouput            [CLS] Token과 Task Specific Parameter의 곱에 Softmax를 취하여 Ouput 추출                  Text Similarity Ouput            [CLS] Token을 활용하여 Task Specific Parameter와 곱한 후 sigmoid function을 사용하여 Score를 예측                  Pairwise Text Classification Ouput            BERT와 다르게 Stochastic Answer Network(SAN)를 이용함                    NLI의 기존 SOTA 모듈, 주어진 문장들에 대한 Multi-step Reasoning을 모델링하는 구조 (한번에 classification 결과를 예측하지 않고 여러번의 예측을 통한 Reasoning으로 결과를 예측)           SAN은 GRU모듈에 주어진 문장쌍의 representation을 Input 및 hidden state로 넣는 과정을 k번 반복함으로써 정제된 representation을 얻고 이를 이용하여 최종 예측                                 각 step을 진행할때마다 linear classifier를 거쳐 각 클래스에 대한 확률 분포 계산         모든 K-step output을 평균하여 클래스에 대한 최종 확률 분포를 계산, averaging 연산 전에 stochastic prediction droput을 적용         Relevance Ranking Ouput            Question과 문장 Pair Input으로 넣어 생성한 [CLS] Token에 Sigmoid를 취하여 문장 별로 점수를 Scoring하고 가장 높은 점수 만 Question에 해당하는 정답이 있다고 예측하는 방식              Training Procedure     ","categories": ["NLP"],
        "tags": ["paper review","nlp"],
        "url": "/nlp/NLP_04/",
        "teaser": null
      }]
