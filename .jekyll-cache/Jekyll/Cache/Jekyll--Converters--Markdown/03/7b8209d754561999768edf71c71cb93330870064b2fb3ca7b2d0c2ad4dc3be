I"Š	<h1 id="introduction">Introduction</h1>
<ul>
  <li>Pre-trained word representations are a key component in many neural language understanding models.</li>
  <li>Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence.</li>
  <li>We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus.</li>
  <li>In many experiments, the ELMo representation has been shown to be very excellent, and the error rate is relatively reduced by 20%</li>
</ul>

<h1 id="related-work">Related Work</h1>
<ul>
  <li>Pretrained word vectorì˜ í™œìš©ì´ í‘œì¤€í™” ë˜ì—ˆì§€ë§Œ, í•˜ë‚˜ì˜ ë‹¨ì–´ì— í•˜ë‚˜ì˜ ë²¡í„°ë¥¼ ë¶€ì—¬í•˜ë‹¤ë³´ë‹ˆ context-independentí•œ ë¬¸ì œê°€ ìˆì—ˆë‹¤.</li>
  <li>ì›Œë“œ ì„ë² ë”©ì„ í’ë¶€í•˜ê²Œ í•˜ê¸° ìœ„í•´, subword informationì„ í™œìš©í•˜ê±°ë‚˜ ë‹¤ì˜ì–´ì˜ ê²½ìš° ì˜ë¯¸ë³„ë¡œ ë‹¤ë¥¸ ë²¡í„°ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì´ ë“±ì¥í•˜ì˜€ë‹¤.
    <ul>
      <li>context2vec</li>
      <li>CoVe</li>
    </ul>
  </li>
  <li>ì´ì „ ì—°êµ¬ì— ì˜í•˜ë©´ biRNNì˜ ì„œë¡œ ë‹¤ë¥¸ ë ˆì´ì–´ê°€ ë‹¤ë¥¸ í˜•íƒœì˜ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ëŠ”ë°, ë³¸ ì—°êµ¬ì—ì„œë„ ìœ ì‚¬í•œ íš¨ê³¼ê°€ ë‚˜íƒ€ë‚¬ë‹¤.</li>
</ul>

<h1 id="model">Model</h1>
<ul>
  <li>ELMo
    <ul>
      <li>ELMo word representations are functions of the entire input sentence.</li>
      <li>They are computed on top of two-layer biLMs with character convolutions, as a linear function of the internal network states.</li>
      <li>This setup allows us to do semi-supervised learning, where the biLM is pre-trained at a large scale and easily incorporated into a wide range of existing neural NLP architectures.</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/05/img.png" width="80%" height="100%" /></p>

<ol>
  <li>Pretrained BiLM</li>
</ol>

<p><img src="../../assets/img/nlp/05/img_1.png" width="80%" height="100%" /></p>

<p><img src="../../assets/img/nlp/05/img_2.png" width="80%" height="100%" /></p>

<ol>
  <li>Task-specific ELMo Embedding</li>
</ol>

<p><img src="../../assets/img/nlp/05/img_3.png" width="80%" height="100%" /></p>

<p><img src="../../assets/img/nlp/05/img_4.png" width="80%" height="100%" /></p>

<ol>
  <li>Using biLMs for supervised NLP tasks</li>
</ol>

<h1 id="evaluation">Evaluation</h1>
<h1 id="analysis">Analysis</h1>
<h1 id="conclusion">Conclusion</h1>
:ET