I"Î<h1 id="introduction">Introduction</h1>
<ul>
  <li>Pre-trained word representations are a key component in many neural language understanding models.</li>
  <li>Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence.</li>
  <li>We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus.</li>
  <li>In many experiments, the ELMo representation has been shown to be very excellent, and the error rate is relatively reduced by 20%</li>
</ul>

<p><br /><br /></p>
<h1 id="related-works">Related Works</h1>
<ul>
  <li>Pretrained word vectorì˜ í™œìš©ì´ í‘œì¤€í™” ë˜ì—ˆì§€ë§Œ, í•˜ë‚˜ì˜ ë‹¨ì–´ì— í•˜ë‚˜ì˜ ë²¡í„°ë¥¼ ë¶€ì—¬í•˜ë‹¤ë³´ë‹ˆ context-independentí•œ ë¬¸ì œê°€ ìˆì—ˆë‹¤.</li>
  <li>ì›Œë“œ ì„ë² ë”©ì„ í’ë¶€í•˜ê²Œ í•˜ê¸° ìœ„í•´, subword informationì„ í™œìš©í•˜ê±°ë‚˜ ë‹¤ì˜ì–´ì˜ ê²½ìš° ì˜ë¯¸ë³„ë¡œ ë‹¤ë¥¸ ë²¡í„°ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì´ ë“±ì¥í•˜ì˜€ë‹¤.
    <ul>
      <li>context2vec</li>
      <li>CoVe</li>
    </ul>
  </li>
  <li>ì´ì „ ì—°êµ¬ì— ì˜í•˜ë©´ biRNNì˜ ì„œë¡œ ë‹¤ë¥¸ ë ˆì´ì–´ê°€ ë‹¤ë¥¸ í˜•íƒœì˜ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ëŠ”ë°, ë³¸ ì—°êµ¬ì—ì„œë„ ìœ ì‚¬í•œ íš¨ê³¼ê°€ ë‚˜íƒ€ë‚¬ë‹¤.</li>
</ul>

<p><br /><br /></p>
<h1 id="model">Model</h1>
<ul>
  <li>ELMO
    <ul>
      <li>ELMO word representations are functions of the entire input sentence.</li>
      <li>They are computed on top of two-layer biLMs with character convolutions, as a linear function of the internal network states.</li>
      <li>This setup allows us to do semi-supervised learning, where the biLM is pre-trained at a large scale and easily incorporated into a wide range of existing neural NLP architectures.</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/03/img.png" width="100%" height="100%" /></p>

<ul>
  <li>Pretrained BiLM</li>
</ul>

<p><img src="../../assets/img/nlp/03/img_1.png" width="100%" height="100%" />
<img src="../../assets/img/nlp/03/img_2.png" width="100%" height="100%" /></p>

<ul>
  <li>Task-specific ELMo Embedding</li>
</ul>

<p><img src="../../assets/img/nlp/03/img_3.png" width="100%" height="100%" /></p>

<ul>
  <li>Using biLMs for supervised NLP tasks
    <ul>
      <li>ê¸°ì¡´ì˜ ì„ë² ë”© ë²¡í„°ì™€ í•¨ê»˜ ì‚¬ìš©ëœë‹¤.</li>
      <li>ELMo í‘œí˜„ì„ ë§Œë“œëŠ”ë° ì‚¬ìš©ëœ ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ëŠ” ê³ ì •ì‹œí‚¤ê³ , ê° ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ ìŠ¤ì¹¼ë¼ íŒŒë¼ë¯¸í„°ëŠ” í›ˆë ¨ ê³¼ì •ì—ì„œ í•™ìŠµëœë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/03/img_4.png" width="100%" height="100%" /></p>

<p><br /><br /></p>
<h1 id="evaluation">Evaluation</h1>
<ul>
  <li>6ê°œì˜ NLP taskì—ì„œ ì—ëŸ¬ìœ¨ì„ 6~20% ì¤„ì˜€ë‹¤.</li>
  <li>6ê°œì˜ NLP taskì—ì„œ ë†’ì€ ì ìˆ˜ë¥¼ ê¸°ë¡í–ˆë‹¤.</li>
</ul>

<p><br /><br /></p>
<h1 id="experimentunsupervised-machine-translation">Experiment:Unsupervised Machine Translation</h1>
<p><img src="../../assets/img/nlp/02/img_5.png" width="100%" height="100%" /></p>
<ul>
  <li>Unsupervised Machine Translation Taskì— ìˆì–´ ì´ì „ ëª¨ë¸ë“¤, ìµœê·¼ê¹Œì§€ SOTAì˜€ë˜ Facebookì˜ XLMëª¨ë¸ê³¼ MASSì˜ ì„±ëŠ¥ ë¹„êµë¥¼ ìˆ˜í–‰</li>
  <li>Facebookì˜ XLMì€ Encoderì™€ Decoderë¥¼ ê°ê° Bertì˜ Masked LMê³¼ standard LMìœ¼ë¡œ Pre-train ì‹œí‚¨ ëª¨ë¸</li>
  <li>MASSëŠ” 6ê°œì˜ Machine Translation Taskì— ìˆì–´ XLMì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥, SOTA ê¸°ë¡</li>
</ul>

<p><br /><br /></p>

<h1 id="experimentlow-resource-translation">Experiment:Low-resource Translation</h1>
<p><img src="../../assets/img/nlp/02/img_6.png" width="100%" height="100%" /></p>
<ul>
  <li>Low-resource Machine Translation : bilingual íŠ¸ë ˆì´ë‹ ë°ì´í„°ì…‹ì´ ë¶€ì¡±í•œ í™˜ê²½ì—ì„œì˜ ê¸°ê³„ë²ˆì—­</li>
  <li>Englisgh-Fench, English-German, English-Romanian ë°ì´í„° ì…‹ì„ 10K, 100K, 1Mì˜ ì‚¬ì´ì¦ˆë¡œ ëŠ˜ë ¤ê°€ë©° Low-resource í™˜ê²½ì—ì„œì˜ Machine Translation í…ŒìŠ¤íŠ¸</li>
  <li>MASSëŠ” ëª¨ë“  ìŠ¤ì¼€ì¼ì—ì„œ Low-resource MTì˜ baseline ì„±ëŠ¥ì„ ëŠ¥ê°€</li>
  <li>ë°ì´í„°ì…‹ì˜ ì‚¬ì´ì¦ˆê°€ ì‘ì„ìˆ˜ë¡ ë”ìš± ë‘ë“œëŸ¬ì§€ê²Œ ë‚˜íƒ€ë‚¨</li>
</ul>

<p><br /><br /></p>

<h1 id="experimentabstractive-summarization">Experiment:Abstractive Summarization</h1>
<p><img src="../../assets/img/nlp/02/img_7.png" width="100%" height="100%" /></p>
<ul>
  <li>Pre-trained Bertë¥¼ ì¸ì½”ë”ë¡œ Pre-trained Language Modelì„ Decoderë¡œ ì‚¬ìš©í•œ BERT+LM ëª¨ë¸ê³¼ DAE(Denoising Auto-Encoder), MASSì˜ Abstractive Summarization ì„±ëŠ¥ì„ Gigaword Corpusì— ëŒ€í•´ ë¹„êµ</li>
  <li>MASSëŠ” BERT+LMê³¼ DAE ë‘ ëª¨ë¸ì˜ Abstractive Summarization ì„±ëŠ¥ì„ ëª¨ë‘ ëŠ¥ê°€</li>
</ul>

<p><br /><br /></p>

<h1 id="experimentconversational-response-generation">Experiment:Conversational Response Generation</h1>
<p><img src="../../assets/img/nlp/02/img_8.png" width="100%" height="100%" /></p>
<ul>
  <li>Abstractive Summarization ì„±ëŠ¥ ë¹„êµì— ì‚¬ìš©ë˜ì—ˆë˜ BERT+LM ëª¨ë¸ê³¼ Baseline, MASSì˜ Conversational Response Generation ì„±ëŠ¥ì„ Cornell Movie Dialog Corpusì— ëŒ€í•´ ë¹„êµ</li>
  <li>MASSê°€ BERT+LM ëª¨ë¸ê³¼ Baseline ë³´ë‹¤ ë‚®ì€ Perplexityë¥¼ ê¸°ë¡í•˜ë©° ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ</li>
</ul>

<p><br /><br /></p>

<h1 id="the-probability-formulation">The probability formulation</h1>
<p><img src="../../assets/img/nlp/02/img_9.png" width="100%" height="100%" /></p>
<ul>
  <li>(a) ì˜ì–´-í”„ë‘ìŠ¤ ë²ˆì—­ì˜ ì˜ì–´, (b) ì˜ì–´-í”„ë‘ìŠ¤ ë²ˆì—­ì˜ í”„ë‘ìŠ¤ì–´, (c) ì˜ì–´-í”„ë‘ìŠ¤ ë²ˆì—­ì˜ BLUE score (d) Text Summarizationì˜ ROUGE score, (e) conversational response generationì˜ PPL</li>
  <li>í•˜ì´í¼ íŒŒë¼ë¯¸í„° Kë¥¼ ë‹¤ë¥´ê²Œ ì„¤ì •í•´ê°€ë©°, MASSì˜ ì„±ëŠ¥ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì‹¤í—˜ ê²°ê³¼</li>
  <li>ì‹¤í—˜ì„ í†µí•´ Kê°€ ë¬¸ì¥ì˜ ì ˆë°˜ ì •ë„ í¬ê¸°ì— í•´ë‹¹í•  ë•Œ, downstream taskì—ì„œ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì„ì„ ì•Œê²Œ ë¨
    <ul>
      <li>ë¬¸ì¥ ë‚´ ì ˆë°˜ì˜ í† í°ì„ Maskingí•˜ëŠ” ê²ƒì´ Encoderì™€ Decoderì˜ Pre-trainingì— ìˆì–´ ì ì ˆí•œ ê· í˜•ì„ ì œê³µ</li>
    </ul>
  </li>
  <li>Kê°€ 1(BERT), Kê°€ m(GPT) ì´ì—ˆì„ ë•ŒëŠ” downstream taskì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì´ ë‚˜ì˜¤ì§€ ì•ŠìŒ ì´ ì´ìœ ëŠ” MASSê°€ Sequence to Sequence ê¸°ë°˜ì˜ Language Generation Taskì— ì´ì ì„ ì§€ë‹ˆê³  ìˆìŒì„ ë°˜ì¦</li>
</ul>

<p><br /><br /></p>
<h1 id="couclusion">Couclusion</h1>
<ul>
  <li>MASSëŠ” Sequence to Sequence ê¸°ë°˜ì˜ ë‹¤ì–‘í•œ Language generation Taskì— ìˆì–´ ì¢‹ì€ ì„±ëŠ¥ ê¸°ë¡</li>
  <li>Language generationì—ì„œ ì¢‹ì€ ì„±ëŠ¥ì„ ê¸°ë¡í•œ MASSê°€ BERTì™€ XLnetì´ ë›°ì–´ë‚œ ê²°ê³¼ë¥¼ ê¸°ë¡í•œ Natural Language Understanding Taskì—ì„œë„ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¼ ê²ƒì¸ì§€ ì°¨í›„ ì‹¤í—˜í•´ë³¼ ì˜ˆì •</li>
  <li>ë˜í•œ MASS ëª¨ë¸ì´ Imageì™€ Videoì™€ ê°™ì€ NLPì™€ ë‹¤ë¥¸ ë„ë©”ì¸ì—ì„œë„ Sequence to Sequence ê¸°ë°˜ì˜ ìƒì„± Taskë¥¼ ìˆ˜í–‰í•´ë‚¼ ìˆ˜ ìˆì„ì§€ì— ëŒ€í•œ ì¶”ê°€ ì‹¤í—˜ë„ í•´ë³¼ ì˜ˆì •</li>
</ul>
:ET