I"<h1 id="introduction">Introduction</h1>
<ul>
  <li>Pre-trainingì€ ìµœê·¼ NLP ë¶„ì•¼ì—ì„œ í™œë°œí•œ ì—°êµ¬ê°€ ì´ë£¨ì–´ì§€ëŠ” ì£¼ì œ ì¤‘ í•˜ë‚˜</li>
  <li>Googleì˜ Bertì™€ XLNetì€ ìì—°ì–´ ì´í•´(NLU) Taskì—ì„œ í° ì„±ê³µì„ ê±°ë‘” ëª¨ë¸</li>
  <li>NLU Task ì™¸ì— Sequence to Sequenceë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ìì—°ì–´ ìƒì„± Taskê°€ ìˆìŒ
    <ul>
      <li>Machine Translation, Abstract Text Summarization</li>
    </ul>
  </li>
  <li>ìì—°ì–´ ìƒì„± TaskëŠ” Encoder - Attention - Decoder êµ¬ì¡°ì˜ ëª¨ë¸ë“¤ì´ ì‚¬ìš©ë¨</li>
</ul>

<h1 id="encoder-attention-decoder">Encoder-Attention-Decoder</h1>
<p><img src="../../assets/img/nlp/02/img.png" width="100%" height="100%" /></p>
<ul>
  <li>EncoderëŠ” Source sequence Xë¥¼ inputìœ¼ë¡œ ë°›ì€ í›„, ì´ë¥¼ hidden representations sequenceë¡œ ë³€í˜•</li>
  <li>DecoderëŠ” Encoderë¡œë¶€í„° hidden representations sequenceë¥¼ ì „ë‹¬ ë°›ì•„ Target Sequence Yë¥¼ ìƒì„±í•˜ë©°, ì´ ë•Œ Attention mechanismì´ í•™ìŠµí•œ Inputì— ëŒ€í•œ ì •ë³´ë¥¼ í•¨ê»˜ ì°¸ì¡°</li>
</ul>

<h1 id="related-works">Related Works</h1>
<p><img src="../../assets/img/nlp/02/img_1.png" width="100%" height="100%" /></p>
<ul>
  <li>Bertì™€ XLNetì€ Language Understandingì„ ìœ„í•´ Encoderë¥¼ Pre-train</li>
  <li>GPTëŠ” Language Modelingì„ ìœ„í•´ Decoderë¥¼ Pre-train</li>
  <li>ì´ì „ ëª¨ë¸ë“¤ì€ Encoder, Attention, Decoderë¥¼ í•¨ê»˜ Pre-trainí•˜ì§€ ëª»í–ˆìŒ</li>
</ul>

<h1 id="massmasked-sequence-to-sequence-pre-training">MASS:Masked Sequence to Sequence Pre-training</h1>
<p><img src="../../assets/img/nlp/02/img_2.png" width="100%" height="100%" /></p>
<ul>
  <li>ìƒˆë¡œìš´ ëª¨ë¸ MASSëŠ” Masked Sequence to Sequence Pre-trainingì˜ ì•½ì</li>
  <li>Inputì—ì„œ Kê°œì˜ í† í° Fragmentë¥¼ ì„ì˜ë¡œ ì§€ì •í•´ Masking</li>
  <li>ë§ˆìŠ¤í‚¹ ëœ Fragmentë¥¼ Encoder-Attention-Decoderë¥¼ ê±°ì³ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ì‹œí‚´</li>
  <li>Encoderì—ì„œ Maskingë˜ì§€ ì•Šì€ í† í°ë“¤ì´ Decoderì—ì„œ Maskingë¨ì— ë”°ë¼, DecoderëŠ” Encoderê°€ ì œê³µí•œ hidden representationê³¼ Attention ì •ë³´ë§Œì„ ì°¸ê³ í•˜ì—¬ Maskingëœ í† í°ë“¤ì„ ì˜ˆì¸¡í•´ì•¼í•¨, ì´ë¥¼ í†µí•´ Encoder-Attention-Decoderê°€ í•¨ê»˜ Pre-trainë  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì œê³µ</li>
  <li>EncoderëŠ” Encoderì—ì„œ Maskingë˜ì§€ ì•Šì€ í† í°ë“¤ì˜ ì •ë³´ë¥¼ ì˜ ì¶”ì¶œí•  ìˆ˜ ìˆë„ë¡ í•™ìŠµ, ì´ë¥¼ í†µí•´ Language Understanding ëŠ¥ë ¥ ê°œì„ </li>
  <li>DecoderëŠ” Encoderì—ì„œ Maksingëœ í† í°ë“¤ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì—°ì†ì ìœ¼ë¡œ ìˆ˜í–‰í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— Language Modeling ëŠ¥ë ¥ì„ í•™ìŠµí•˜ê²Œ ë¨</li>
</ul>

<p><img src="../../assets/img/nlp/02/img_3.png" width="100%" height="100%" /></p>
<ul>
  <li>KëŠ” Encoderì—ì„œ Maskingë˜ëŠ” í† í° ê°œìˆ˜ë¥¼ ê²°ì •í•˜ëŠ” í•˜ì´í¼ íŒŒë¼ë¯¸í„°</li>
  <li>í•˜ì´í¼ íŒŒë¼ë¯¸í„° Kë¥¼ ì¡°ì •í•¨ìœ¼ë¡œì¨ MASSëŠ” BERTì˜ Masked LMê³¼ GPTì˜ Standard LMì„ ëª¨ë‘ êµ¬í˜„ ê°€ëŠ¥</li>
  <li>BERT Masked LM
    <ul>
      <li>K=1ì¼ ë•Œ, Encoderì—ì„œëŠ” í•˜ë‚˜ì˜ í† í°ì´ Maskingë˜ê³ , DecoderëŠ” Maskingëœ í•˜ë‚˜ì˜ í† í°ì„ ì˜ˆì¸¡í•´ì•¼ í•˜ë¯€ë¡œ BERTì˜ Masked LMê³¼ ê°™ì•„ì§</li>
    </ul>
  </li>
  <li>GPT Standard LM
    <ul>
      <li>í•˜ì´í¼ íŒŒë¼ë¯¸í„° Kê°€ Input ë¬¸ì¥ì˜ ì „ì²´ ê¸¸ì´ì¸ mê³¼ ê°™ì„ ë•Œ Encoderì˜ ëª¨ë“  í† í°ë“¤ì´ Maskingë¨</li>
      <li>DecoderëŠ” Encoderë¡œë¶€í„° ì–´ë– í•œ ì •ë³´ë„ ë¶€ì—¬ ë°›ì§€ ì•Šì€ ì±„ ëª¨ë“  í† í°ì„ ì˜ˆì¸¡í•´ì•¼ í•˜ë¯€ë¡œ, GPTì˜ Standard LMê³¼ ê°™ì•„ì§</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/02/img_4.png" width="100%" height="100%" /></p>
<ul>
  <li>m - Input sequenceì˜ ì „ì²´ê¸¸ì´</li>
  <li>u - Maskingeëœ Fragmentì˜ ì‹œì‘ì </li>
  <li>v - Maskingëœ Fragmentì˜ ëì  X^u - uë¶€í„° vê¹Œì§€ì˜ fragment X^\u:v - uë¶€í„° vê¹Œì§€ Makingëœ Input Sequence</li>
</ul>
:ET