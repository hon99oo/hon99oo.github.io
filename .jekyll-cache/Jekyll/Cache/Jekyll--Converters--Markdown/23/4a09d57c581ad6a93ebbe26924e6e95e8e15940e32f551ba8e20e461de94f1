I"<h1 id="introduction">Introduction</h1>
<ul>
  <li>Pre-training은 최근 NLP 분야에서 활발한 연구가 이루어지는 주제 중 하나</li>
  <li>Google의 Bert와 XLNet은 자연어 이해(NLU) Task에서 큰 성공을 거둔 모델</li>
  <li>NLU Task 외에 Sequence to Sequence를 기반으로 하는 자연어 생성 Task가 있음
    <ul>
      <li>Machine Translation, Abstract Text Summarization</li>
    </ul>
  </li>
  <li>자연어 생성 Task는 Encoder - Attention - Decoder 구조의 모델들이 사용됨</li>
</ul>
:ET