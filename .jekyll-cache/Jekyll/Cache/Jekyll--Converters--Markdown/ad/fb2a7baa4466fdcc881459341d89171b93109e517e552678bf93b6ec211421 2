I"<blockquote>
  <p>lecture 5, 7, 13 ,17 을 리뷰 했는데,,, 17강 밖에 안남아있다…T.T</p>
</blockquote>

<h1 id="the-limits-of-single-task-learning">The Limits of Single-task Learning</h1>
<ul>
  <li>Great performance improvements in recent years given
    <ul>
      <li>dataset</li>
      <li>task</li>
      <li>model</li>
      <li>metric</li>
    </ul>
  </li>
  <li>Models typically start from ranbdom or are only partly pre-trained</li>
</ul>

<h1 id="pre-training-and-sharing-knowledge-is-great">Pre-training and sharing knowledge is great!</h1>
<ul>
  <li>Computer Vision
    <ul>
      <li>ImageNet + CNN이 큰 성공을 거두었음</li>
      <li>Classification task가 과거에 큰 장벽이었음</li>
      <li>이 문제가 해결되고 많은 문제들을 푸는 것이 가능해짐</li>
    </ul>
  </li>
  <li>Natural Language Processing
    <ul>
      <li>Word2Vec, Glove</li>
      <li>최근 CoVe, ELMo, BERT 성공을 거두기 시작함</li>
    </ul>
  </li>
</ul>

<h1 id="why-has-weightmodel-sharing-not-happened-as-much-in-nlp">Why has weight&amp;model sharing not happened as much in NLP?</h1>
<ul>
  <li>NLP는 많은 종류의 추론이 요구됨
    <ul>
      <li>logical, linguistic, emotional, visual</li>
    </ul>
  </li>
  <li>Short and long term memory가 요구됨</li>
  <li>NLP는 중간 단계 또는 분리된 Task로 많이 나누어져 있음</li>
  <li>하나의 Unsupervised Task가 전체 문제를 해결할 수 없음</li>
  <li>언어는 현실적으로 분명한 Supervision이 필요함</li>
</ul>

<h1 id="why-a-unified-multi-task-model-for-nlp">Why a unified multi-task model for NLP</h1>
<ul>
  <li>Multi-task learning은 General NLP system이 넘어야할 장벽임</li>
  <li>하나의 통합된 모델은 지식을 어떻게 전달할지 결정 가능
    <ul>
      <li>Domain adaptation, weight sharing, transfer and zero shot learning</li>
    </ul>
  </li>
  <li>하나의 통합된 Multi-task 모델은
    <ul>
      <li>새로운 task가 주어졌을 때 쉽게 적응할 수 있음</li>
      <li>실제 production을 위해 deploy하는 것이 매우 간단해짐</li>
      <li>더 많은 사람들이 새로운 task를 해결할 수 있도록 도와줌</li>
      <li>잠재적으로 Continual learning으로 나아갈 수 있음</li>
      <li>모든 프로젝트를 계속 다시 시작하게 된다면 자연 언어의 복잡성을 점점 더 많이 포함하는 하나의 모델에 도달하지 못함</li>
    </ul>
  </li>
  <li>인공지능이 대화를 가능하게 하는 task를 진행할 때 사람의 언어처럼 순차적으로 처리하는 것 만큼 비효율적인 것은 없음. 컴퓨터가 인간의 언어를 supervision하지 않다면 훨씬 더많은 언어로 의사소통 가능.</li>
</ul>

<h1 id="how-to-express-many-nlp-tasks-in-the-same-framework">How to express many NLP tasks in the same framework?</h1>
<ul>
  <li>Sequence tagging
    <ul>
      <li>Named Entity Recognition, aspect specific sentiment</li>
    </ul>
  </li>
  <li>Text classification
    <ul>
      <li>Dialogue state tracking, sentiment classification</li>
    </ul>
  </li>
  <li>Seq2seq
    <ul>
      <li>Machine Translation, Summarization, Question Answering</li>
    </ul>
  </li>
</ul>

<h1 id="three-equivalent-supertasks-of-nlp">Three equivalent Supertasks of NLP</h1>
<ul>
  <li>Language Modeling
    <ul>
      <li>다음 단어를 예측하는 것은 질문의 일종이다.</li>
    </ul>
  </li>
  <li>Question Answering
    <ul>
      <li>말 그대로 Question &amp; Answering 이다.</li>
    </ul>
  </li>
  <li>Dialogue System
    <ul>
      <li>질문 답변, 답변에 대한 답변 형태이다.</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/06/img.png" width="40%" height="100%" /></p>

<h1 id="the-natural-language-decathlondecanlp">The Natural Language Decathlon(decaNLP)</h1>
<ul>
  <li>The Natural Language Decathlon : Multitask Learning as Question Answering
    <ul>
      <li>10개의 NLP Task를 하나의 Question Answering모델 학습만으로 풀 수 있도록 디자인</li>
      <li>이를 위해 모든 데이터 셋에 대하여 question, context, answer 형태로 전처리</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/06/img_1.png" width="100%" height="100%" /></p>

<h1 id="multitask-question-answering-networkmqan">Multitask Question Answering Network(MQAN)</h1>

<p><img src="../../assets/img/nlp/06/img_3.png" width="100%" height="100%" /></p>

<p><img src="../../assets/img/nlp/06/img_4.png" width="100%" height="100%" /></p>

<ol>
  <li>Fixed Glove+Character n-gram embeddings -&gt; Linear -&gt; Shared BiLSTM with skip connection</li>
  <li>Attention summations from one sequence to the other and back again with skip connections</li>
  <li>Separate BiLSTMs to reduce dimensionality, two transformer layers, another BiLSTM</li>
  <li>Auto-regressive decoder :
    <ul>
      <li>Fixed GloVe, character n-gram embeddings</li>
      <li>Two transformer layers</li>
      <li>LSTM Layers (attend last 3 layers of encoder)</li>
    </ul>
  </li>
</ol>

<p><img src="../../assets/img/nlp/06/img_5.png" width="100%" height="100%" /></p>

<p><img src="../../assets/img/nlp/06/img_6.png" width="100%" height="100%" /></p>

<p><img src="../../assets/img/nlp/06/img_7.png" width="100%" height="100%" /></p>

<p><img src="../../assets/img/nlp/06/img_8.png" width="100%" height="100%" /></p>

<p><img src="../../assets/img/nlp/06/img_9.png" width="100%" height="100%" /></p>

<p><img src="../../assets/img/nlp/06/img_10.png" width="100%" height="100%" /></p>

<h1 id="training-strategies-fully-joint">Training Strategies: Fully Joint</h1>

<p><img src="../../assets/img/nlp/06/img_11.png" width="100%" height="100%" /></p>
<ul>
  <li>Curriculum learning : 모델 학습시 전체 데이터를 한번에 학습시키지 않고 쉬운것-&gt;어려운 것 순서로 학습함</li>
  <li>Batch를 Sampling할 때, Fixed order로 계속 Round Robin하여 수집함</li>
  <li>많은 양이 돌아 Converge되는 Task들은 잘 동작하지 않음</li>
</ul>

<h1 id="training-strategies-anti-curriculum-pre-training">Training Strategies: Anti-Curriculum Pre-training</h1>

<p><img src="../../assets/img/nlp/06/img_12.png" width="100%" height="100%" /></p>

<p><img src="../../assets/img/nlp/06/img_13.png" width="100%" height="100%" /></p>

<h1 id="training-strategies-cove">Training Strategies: CoVe</h1>

<p><img src="../../assets/img/nlp/06/img_14.png" width="100%" height="100%" /></p>

<h1 id="whats-next-for-nlp">What’s next for NLP?</h1>

<p><img src="../../assets/img/nlp/06/img_15.png" width="100%" height="100%" /></p>
:ET