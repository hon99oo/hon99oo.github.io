I"‘<h1 id="introduction">Introduction</h1>
<ul>
  <li>Pre-trainingì€ ìµœê·¼ NLP ë¶„ì•¼ì—ì„œ í™œë°œí•œ ì—°êµ¬ê°€ ì´ë£¨ì–´ì§€ëŠ” ì£¼ì œ ì¤‘ í•˜ë‚˜</li>
  <li>Googleì˜ Bertì™€ XLNetì€ ìì—°ì–´ ì´í•´(NLU) Taskì—ì„œ í° ì„±ê³µì„ ê±°ë‘” ëª¨ë¸</li>
  <li>NLU Task ì™¸ì— Sequence to Sequenceë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ìì—°ì–´ ìƒì„± Taskê°€ ìˆìŒ
    <ul>
      <li>Machine Translation, Abstract Text Summarization</li>
    </ul>
  </li>
  <li>ìì—°ì–´ ìƒì„± TaskëŠ” Encoder - Attention - Decoder êµ¬ì¡°ì˜ ëª¨ë¸ë“¤ì´ ì‚¬ìš©ë¨</li>
</ul>

<h1 id="encoder-attention-decoder">Encoder-Attention-Decoder</h1>
<p><img src="../../assets/img/nlp/02/img.png" width="100%" height="100%" /></p>
<ul>
  <li>EncoderëŠ” Source sequence Xë¥¼ inputìœ¼ë¡œ ë°›ì€ í›„, ì´ë¥¼ hidden representations sequenceë¡œ ë³€í˜•</li>
  <li>DecoderëŠ” Encoderë¡œë¶€í„° hidden representations sequenceë¥¼ ì „ë‹¬ ë°›ì•„ Target Sequence Yë¥¼ ìƒì„±í•˜ë©°, ì´ ë•Œ Attention mechanismì´ í•™ìŠµí•œ Inputì— ëŒ€í•œ ì •ë³´ë¥¼ í•¨ê»˜ ì°¸ì¡°</li>
</ul>

<h1 id="related-works">Related Works</h1>
<p><img src="../../assets/img/nlp/02/img_1.png" width="100%" height="100%" /></p>
<ul>
  <li></li>
</ul>
:ET