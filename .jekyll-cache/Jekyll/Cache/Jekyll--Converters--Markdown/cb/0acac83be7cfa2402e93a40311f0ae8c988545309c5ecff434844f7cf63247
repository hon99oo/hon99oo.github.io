I"(<h1 id="introduction">Introduction</h1>
<ul>
  <li>Pre-trainingì€ ìµœê·¼ NLP ë¶„ì•¼ì—ì„œ í™œë°œí•œ ì—°êµ¬ê°€ ì´ë£¨ì–´ì§€ëŠ” ì£¼ì œ ì¤‘ í•˜ë‚˜</li>
  <li>Googleì˜ Bertì™€ XLNetì€ ìì—°ì–´ ì´í•´(NLU) Taskì—ì„œ í° ì„±ê³µì„ ê±°ë‘” ëª¨ë¸</li>
  <li>NLU Task ì™¸ì— Sequence to Sequenceë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•˜ëŠ” ìì—°ì–´ ìƒì„± Taskê°€ ìˆìŒ
    <ul>
      <li>Machine Translation, Abstract Text Summarization</li>
    </ul>
  </li>
  <li>ìì—°ì–´ ìƒì„± TaskëŠ” Encoder - Attention - Decoder êµ¬ì¡°ì˜ ëª¨ë¸ë“¤ì´ ì‚¬ìš©ë¨</li>
</ul>

<h1 id="encoder-attention-decoder">Encoder-Attention-Decoder</h1>
<p><img src="../../assets/img/nlp/02/img.png" width="100%" height="100%" /></p>
<ul>
  <li>EncoderëŠ” Source sequence Xë¥¼ inputìœ¼ë¡œ ë°›ì€ í›„, ì´ë¥¼ hidden representations sequenceë¡œ ë³€í˜•</li>
  <li>DecoderëŠ” Encoderë¡œë¶€í„° hidden representations sequenceë¥¼ ì „ë‹¬ ë°›ì•„ Target Sequence Yë¥¼ ìƒì„±í•˜ë©°, ì´ ë•Œ Attention mechanismì´ í•™ìŠµí•œ Inputì— ëŒ€í•œ ì •ë³´ë¥¼ í•¨ê»˜ ì°¸ì¡°</li>
</ul>

<p><br /><br /></p>
<h1 id="related-works">Related Works</h1>
<p><img src="../../assets/img/nlp/02/img_1.png" width="100%" height="100%" /></p>
<ul>
  <li>Bertì™€ XLNetì€ Language Understandingì„ ìœ„í•´ Encoderë¥¼ Pre-train</li>
  <li>GPTëŠ” Language Modelingì„ ìœ„í•´ Decoderë¥¼ Pre-train</li>
  <li>ì´ì „ ëª¨ë¸ë“¤ì€ Encoder, Attention, Decoderë¥¼ í•¨ê»˜ Pre-trainí•˜ì§€ ëª»í–ˆìŒ</li>
</ul>

<p><br /><br /></p>
<h1 id="massmasked-sequence-to-sequence-pre-training">MASS:Masked Sequence to Sequence Pre-training</h1>
<p><img src="../../assets/img/nlp/02/img_2.png" width="100%" height="100%" /></p>
<ul>
  <li>ìƒˆë¡œìš´ ëª¨ë¸ MASSëŠ” Masked Sequence to Sequence Pre-trainingì˜ ì•½ì</li>
  <li>Inputì—ì„œ Kê°œì˜ í† í° Fragmentë¥¼ ì„ì˜ë¡œ ì§€ì •í•´ Masking</li>
  <li>ë§ˆìŠ¤í‚¹ ëœ Fragmentë¥¼ Encoder-Attention-Decoderë¥¼ ê±°ì³ ì˜ˆì¸¡í•˜ë„ë¡ í›ˆë ¨ì‹œí‚´</li>
  <li>Encoderì—ì„œ Maskingë˜ì§€ ì•Šì€ í† í°ë“¤ì´ Decoderì—ì„œ Maskingë¨ì— ë”°ë¼, DecoderëŠ” Encoderê°€ ì œê³µí•œ hidden representationê³¼ Attention ì •ë³´ë§Œì„ ì°¸ê³ í•˜ì—¬ Maskingëœ í† í°ë“¤ì„ ì˜ˆì¸¡í•´ì•¼í•¨, ì´ë¥¼ í†µí•´ Encoder-Attention-Decoderê°€ í•¨ê»˜ Pre-trainë  ìˆ˜ ìˆëŠ” í™˜ê²½ì„ ì œê³µ</li>
  <li>EncoderëŠ” Encoderì—ì„œ Maskingë˜ì§€ ì•Šì€ í† í°ë“¤ì˜ ì •ë³´ë¥¼ ì˜ ì¶”ì¶œí•  ìˆ˜ ìˆë„ë¡ í•™ìŠµ, ì´ë¥¼ í†µí•´ Language Understanding ëŠ¥ë ¥ ê°œì„ </li>
  <li>DecoderëŠ” Encoderì—ì„œ Maksingëœ í† í°ë“¤ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì—°ì†ì ìœ¼ë¡œ ìˆ˜í–‰í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— Language Modeling ëŠ¥ë ¥ì„ í•™ìŠµí•˜ê²Œ ë¨</li>
</ul>

<p><img src="../../assets/img/nlp/02/img_3.png" width="100%" height="100%" /></p>
<ul>
  <li>KëŠ” Encoderì—ì„œ Maskingë˜ëŠ” í† í° ê°œìˆ˜ë¥¼ ê²°ì •í•˜ëŠ” í•˜ì´í¼ íŒŒë¼ë¯¸í„°</li>
  <li>í•˜ì´í¼ íŒŒë¼ë¯¸í„° Kë¥¼ ì¡°ì •í•¨ìœ¼ë¡œì¨ MASSëŠ” BERTì˜ Masked LMê³¼ GPTì˜ Standard LMì„ ëª¨ë‘ êµ¬í˜„ ê°€ëŠ¥</li>
  <li>BERT Masked LM
    <ul>
      <li>K=1ì¼ ë•Œ, Encoderì—ì„œëŠ” í•˜ë‚˜ì˜ í† í°ì´ Maskingë˜ê³ , DecoderëŠ” Maskingëœ í•˜ë‚˜ì˜ í† í°ì„ ì˜ˆì¸¡í•´ì•¼ í•˜ë¯€ë¡œ BERTì˜ Masked LMê³¼ ê°™ì•„ì§</li>
    </ul>
  </li>
  <li>GPT Standard LM
    <ul>
      <li>í•˜ì´í¼ íŒŒë¼ë¯¸í„° Kê°€ Input ë¬¸ì¥ì˜ ì „ì²´ ê¸¸ì´ì¸ mê³¼ ê°™ì„ ë•Œ Encoderì˜ ëª¨ë“  í† í°ë“¤ì´ Maskingë¨</li>
      <li>DecoderëŠ” Encoderë¡œë¶€í„° ì–´ë– í•œ ì •ë³´ë„ ë¶€ì—¬ ë°›ì§€ ì•Šì€ ì±„ ëª¨ë“  í† í°ì„ ì˜ˆì¸¡í•´ì•¼ í•˜ë¯€ë¡œ, GPTì˜ Standard LMê³¼ ê°™ì•„ì§</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/02/img_4.png" width="100%" height="100%" /></p>
<ul>
  <li>m - Input sequenceì˜ ì „ì²´ê¸¸ì´</li>
  <li>u - Maskingeëœ Fragmentì˜ ì‹œì‘ì </li>
  <li>v - Maskingëœ Fragmentì˜ ëì </li>
  <li>X^u - uë¶€í„° vê¹Œì§€ì˜ fragment</li>
  <li>X^\u:v - uë¶€í„° vê¹Œì§€ Makingëœ Input Sequence</li>
</ul>

<p><br /><br /></p>
<h1 id="experiments">Experiments</h1>
<ul>
  <li>Model Configuration
    <ul>
      <li>1024 embeding/hidden size, 4096 feed-forward filter sizeë¥¼ ê°€ì§„ 6-layer encoder, 6-layer decoderë¡œ êµ¬ì„±ëœ ê¸°ë³¸ ëª¨ë¸ êµ¬ì¡° Transformer</li>
    </ul>
  </li>
  <li>Datasets
    <ul>
      <li>2007 - 2017 WMT News Crawl datasets 190M English, 62M French, German 270M</li>
      <li>MASSì˜ íš¨ê³¼ ê²€ì¦ì„ ìœ„í•œ low-resource language Romanian</li>
    </ul>
  </li>
  <li>Pre-training Details
    <ul>
      <li>Fragment length Kì˜ ê¸¸ì´ë¥¼ ë¬¸ì¥ì— ìˆëŠ” ì´ í† í° ìˆ˜ì˜ ì•½ 50%ë¡œ ì„¤ì •í•˜ê³  ì •í™•ì„±ì˜ ë³€í™”ë¥¼ ë¹„êµí•˜ê¸° ìœ„í•˜ì—¬ Kë¥¼ ë°”ê¿”ê°€ë©° ì‹¤í—˜</li>
    </ul>
  </li>
</ul>

<p><br /><br /></p>
<h1 id="experimentunsupervised-machine-translation">Experiment:Unsupervised Machine Translation</h1>
<p><img src="../../assets/img/nlp/02/img_5.png" width="100%" height="100%" /></p>
<ul>
  <li>Unsupervised Machine Translation Taskì— ìˆì–´ ì´ì „ ëª¨ë¸ë“¤, ìµœê·¼ê¹Œì§€ SOTAì˜€ë˜ Facebookì˜ XLMëª¨ë¸ê³¼ MASSì˜ ì„±ëŠ¥ ë¹„êµë¥¼ ìˆ˜í–‰</li>
  <li>Facebookì˜ XLMì€ Encoderì™€ Decoderë¥¼ ê°ê° Bertì˜ Masked LMê³¼ standard LMìœ¼ë¡œ Pre-train ì‹œí‚¨ ëª¨ë¸</li>
  <li>MASSëŠ” 6ê°œì˜ Machine Translation Taskì— ìˆì–´ XLMì„ ëŠ¥ê°€í•˜ëŠ” ì„±ëŠ¥, SOTA ê¸°ë¡</li>
</ul>

<h1 id="experimentlow-resource-translation">Experiment:Low-resource Translation</h1>
<p><img src="../../assets/img/nlp/02/img_6.png" width="100%" height="100%" /></p>
<ul>
  <li>Low-resource Machine Translation : bilingual íŠ¸ë ˆì´ë‹ ë°ì´í„°ì…‹ì´ ë¶€ì¡±í•œ í™˜ê²½ì—ì„œì˜ ê¸°ê³„ë²ˆì—­</li>
  <li>Englisgh-Fench, English-German, English-Romanian ë°ì´í„° ì…‹ì„ 10K, 100K, 1Mì˜ ì‚¬ì´ì¦ˆë¡œ ëŠ˜ë ¤ê°€ë©° Low-resource í™˜ê²½ì—ì„œì˜ Machine Translation í…ŒìŠ¤íŠ¸</li>
  <li>MASSëŠ” ëª¨ë“  ìŠ¤ì¼€ì¼ì—ì„œ Low-resource MTì˜ baseline ì„±ëŠ¥ì„ ëŠ¥ê°€</li>
  <li>ë°ì´í„°ì…‹ì˜ ì‚¬ì´ì¦ˆê°€ ì‘ì„ìˆ˜ë¡ ë”ìš± ë‘ë“œëŸ¬ì§€ê²Œ ë‚˜íƒ€ë‚¨</li>
</ul>

<h1 id="experimentabstractive-summarization">Experiment:Abstractive Summarization</h1>
<p><img src="../../assets/img/nlp/02/img_7.png" width="100%" height="100%" /></p>
<ul>
  <li>Pre-trained Bertë¥¼ ì¸ì½”ë”ë¡œ Pre-trained Language Modelì„ Decoderë¡œ ì‚¬ìš©í•œ BERT+LM ëª¨ë¸ê³¼ DAE(Denoising Auto-Encoder), MASSì˜ Abstractive Summarization ì„±ëŠ¥ì„ Gigaword Corpusì— ëŒ€í•´ ë¹„êµ</li>
  <li>MASSëŠ” BERT+LMê³¼ DAE ë‘ ëª¨ë¸ì˜ Abstractive Summarization ì„±ëŠ¥ì„ ëª¨ë‘ ëŠ¥ê°€</li>
</ul>

<h1 id="experimentconversational-response-generation">Experiment:Conversational Response Generation</h1>
<p><img src="../../assets/img/nlp/02/img_8.png" width="100%" height="100%" /></p>
<ul>
  <li>Abstractive Summarization ì„±ëŠ¥ ë¹„êµì— ì‚¬ìš©ë˜ì—ˆë˜ BERT+LM ëª¨ë¸ê³¼ Baseline, MASSì˜ Conversational Response Generation ì„±ëŠ¥ì„ Cornell Movie Dialog Corpusì— ëŒ€í•´ ë¹„êµ</li>
  <li>MASSê°€ BERT+LM ëª¨ë¸ê³¼ Baseline ë³´ë‹¤ ë‚®ì€ Perplexityë¥¼ ê¸°ë¡í•˜ë©° ë” ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì¤Œ</li>
</ul>

<h1 id="the-probability-formulation">The probability formulation</h1>
<p><img src="../../assets/img/nlp/02/img_9.png" width="100%" height="100%" /></p>
<ul>
  <li></li>
</ul>
:ET