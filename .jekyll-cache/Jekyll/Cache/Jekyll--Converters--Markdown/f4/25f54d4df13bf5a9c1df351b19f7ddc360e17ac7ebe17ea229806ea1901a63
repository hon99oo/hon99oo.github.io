I"2<h1 id="introduction">Introduction</h1>
<ul>
  <li>Pre-trained word representations are a key component in many neural language understanding models.</li>
  <li>Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence.</li>
  <li>We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus.</li>
  <li>In many experiments, the ELMo representation has been shown to be very excellent, and the error rate is relatively reduced by 20%</li>
</ul>

<h1 id="related-work">Related Work</h1>
<ul>
  <li>Pretrained word vector의 활용이 표준화 되었지만, 하나의 단어에 하나의 벡터를 부여하다보니 context-independent한 문제가 있었다.</li>
  <li>워드 임베딩을 풍부하게 하기 위해, subword information을 활용하거나 다의어의 경우 의미별로 다른 벡터를 학습시키는 방법이 등장하였다.
    <ul>
      <li>context2vec</li>
      <li>CoVe</li>
    </ul>
  </li>
  <li>이전 연구에 의하면 biRNN의 서로 다른 레이어가 다른 형태의 정보를 인코딩하는데, 본 연구에서도 유사한 효과가 나타났다.</li>
</ul>

<h1 id="model">Model</h1>
<ul>
  <li>ELMo
    <ul>
      <li>ELMo word representations are functions of the entire input sentence.</li>
      <li>They are computed on top of two-layer biLMs with character convolutions, as a linear function of the internal network states.</li>
      <li>This setup allows us to do semi-supervised learning, where the biLM is pre-trained at a large scale and easily incorporated into a wide range of existing neural NLP architectures.</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/05/img.png" width="80%" height="100%" /></p>

<ol>
  <li>Pretrained BiLM</li>
</ol>

<p><img src="../../assets/img/nlp/05/img_1.png" width="80%" height="100%" /></p>

<p><img src="../../assets/img/nlp/05/img_2.png" width="80%" height="100%" /></p>

<ol>
  <li>Task-specific ELMo Embedding</li>
</ol>

<p><img src="../../assets/img/nlp/05/img_3.png" width="80%" height="100%" /></p>

<p><img src="../../assets/img/nlp/05/img_4.png" width="80%" height="100%" /></p>

<ol>
  <li>Using biLMs for supervised NLP tasks
    <ul>
      <li>기존의 임베딩 벡터와 함께 사용된다.</li>
      <li>ELMo 표현을 만드는데 사용된 사전 훈련된 언어 모델의 가중치는 고정시키고, 각 층의 가중치와 스칼라 파라미터는 훈련 과정에서 학습된다.</li>
    </ul>
  </li>
</ol>

<p><img src="../../assets/img/nlp/05/img_5.png" width="80%" height="100%" /></p>

<h1 id="evaluation">Evaluation</h1>

<p><img src="../../assets/img/nlp/05/img_6.png" width="80%" height="100%" /></p>
<ul>
  <li>6개의 NLP task에서 에러율을 6~20% 줄였다.</li>
  <li>6개의 NLP task에서 높은 점수를 기록했다.</li>
</ul>

<h1 id="analysis">Analysis</h1>

<p><img src="../../assets/img/nlp/05/img_7.png" width="80%" height="100%" /></p>
<ul>
  <li>일부 task(SNLI, SQuAD) 에서는 ELMo 벡터를 Output에 다시 concat 시키는 것이 효과가 있다.
<br /><br /></li>
</ul>

<p><img src="../../assets/img/nlp/05/img_8.png" width="80%" height="100%" />
<br /><br /></p>

<p><br /><br /></p>
<h1 id="conclusion">Conclusion</h1>
:ET