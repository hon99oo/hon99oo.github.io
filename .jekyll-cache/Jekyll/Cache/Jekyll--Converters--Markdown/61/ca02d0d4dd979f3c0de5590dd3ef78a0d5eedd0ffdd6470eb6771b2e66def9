I"	<h1 id="introduction">Introduction</h1>
<ul>
  <li>Multi-Task Learning(MTL)은 새로운 Task를 학습하는 데 도움이 되도록 이전 작업에서 학습된 지식을 적용하는 인간 학습 활동에서 영감을 받음</li>
  <li>Deep neural networks(DNN)를 이용한 representation learning에 MTL을 적용하는 것에 대한 관심이 높아지고 있음
    <ul>
      <li>DNN을 이용한 representation learning은 많은 양의 데이터를 요구함, MTL은 많은 task에서의 supervised labeled data를 제공함</li>
      <li>MTL은 특정 Task에 Overfitting 되지 않도록 Regularization 효과를 줌</li>
    </ul>
  </li>
  <li>MTL과 대조적으로, Language Model은 대용량의 unsupervised dataset을 활용하여 모델을 학습함
    <ul>
      <li>ELMo, GPT, BERT</li>
    </ul>
  </li>
  <li>MT-DNN은 Language Model Pre-Training을 활용한 BERT에 Multi-task learning을 적용하여 성능을 개선한 모델
<br /><br /></li>
</ul>

<h1 id="tasks">Tasks</h1>
<ul>
  <li>GLUE의 9개 task를 MTL에 활용</li>
  <li>Single Sentence Classification
    <ul>
      <li>하나의 문장이 주어졌을 때 문장의 Class를 분류하는 Task</li>
      <li>CoLA : 문장이 문법적으로 맞는지 분류 (True/False)</li>
      <li>SST-2 : 영화 Review 문장의 감정 분류 (Poistive/Negative)</li>
    </ul>
  </li>
  <li>Text Similarity
    <ul>
      <li>문장 쌍이 주어졌을 때, 점수를 예측하는 Regression Task</li>
      <li>STB-B : 문장 간의 의미적 유사도를 점수로 예측</li>
    </ul>
  </li>
  <li>Pairwise Text Classification
    <ul>
      <li>문장 쌍이 주어졌을 때, 문장의 관계를 분류하는 Task</li>
      <li>RTE, MNLI : 문장 간의 의미적 관계를 3가지로 분류 (Entailment, Contradiction, Neutral)
– QQP, MRPC : 문장 간 의미가 같음 여부를 분류 (True/False)
– Relevance Ranking
– QNLI : 질문과 해당 지문 중 한 문장이 쌍으로 주어졌을 때 해당 지문 문장에 질문의 답이 있는지 여부를 분류 (True/False)
– MT-DNN에서는 이를 Rank 방식으로 바꾸어 모든 지문 문장에 정답이 있을 가능성을 Scoring 하여 가장 Score가 높은 지문 문장을 True로 분류하는 방식으로 Task 수행
<br /><br /></li>
    </ul>
  </li>
</ul>

<h1 id="model-architecture">Model Architecture</h1>

<p><img src="../../assets/img/nlp/04/img.png" width="80%" height="100%" /></p>
:ET