I"<h1 id="introduction">Introduction</h1>
<ul>
  <li>Pre-training은 최근 NLP 분야에서 활발한 연구가 이루어지는 주제 중 하나</li>
  <li>Google의 Bert와 XLNet은 자연어 이해(NLU) Task에서 큰 성공을 거둔 모델</li>
  <li>NLU Task 외에 Sequence to Sequence를 기반으로 하는 자연어 생성 Task가 있음
    <ul>
      <li>Machine Translation, Abstract Text Summarization</li>
    </ul>
  </li>
  <li>자연어 생성 Task는 Encoder - Attention - Decoder 구조의 모델들이 사용됨</li>
</ul>

<h1 id="encoder-attention-decoder">Encoder-Attention-Decoder</h1>
<p><img src="../../assets/img/nlp/02/img.png" width="100%" height="100%" /></p>
<ul>
  <li>Encoder는 Source sequence X를 input으로 받은 후, 이를 hidden representations sequence로 변형</li>
  <li>Decoder는 Encoder로부터 hidden representations sequence를 전달 받아 Target Sequence Y를 생성하며, 이 때 Attention mechanism이 학습한 Input에 대한 정보를 함께 참조</li>
</ul>

<p><br /><br /></p>
<h1 id="related-works">Related Works</h1>
<p><img src="../../assets/img/nlp/02/img_1.png" width="100%" height="100%" /></p>
<ul>
  <li>Bert와 XLNet은 Language Understanding을 위해 Encoder를 Pre-train</li>
  <li>GPT는 Language Modeling을 위해 Decoder를 Pre-train</li>
  <li>이전 모델들은 Encoder, Attention, Decoder를 함께 Pre-train하지 못했음</li>
</ul>

<p><br /><br /></p>
<h1 id="massmasked-sequence-to-sequence-pre-training">MASS:Masked Sequence to Sequence Pre-training</h1>
<p><img src="../../assets/img/nlp/02/img_2.png" width="100%" height="100%" /></p>
<ul>
  <li>새로운 모델 MASS는 Masked Sequence to Sequence Pre-training의 약자</li>
  <li>Input에서 K개의 토큰 Fragment를 임의로 지정해 Masking</li>
  <li>마스킹 된 Fragment를 Encoder-Attention-Decoder를 거쳐 예측하도록 훈련시킴</li>
  <li>Encoder에서 Masking되지 않은 토큰들이 Decoder에서 Masking됨에 따라, Decoder는 Encoder가 제공한 hidden representation과 Attention 정보만을 참고하여 Masking된 토큰들을 예측해야함, 이를 통해 Encoder-Attention-Decoder가 함께 Pre-train될 수 있는 환경을 제공</li>
  <li>Encoder는 Encoder에서 Masking되지 않은 토큰들의 정보를 잘 추출할 수 있도록 학습, 이를 통해 Language Understanding 능력 개선</li>
  <li>Decoder는 Encoder에서 Maksing된 토큰들에 대한 예측을 연속적으로 수행해야 하기 때문에 Language Modeling 능력을 학습하게 됨</li>
</ul>

<p><img src="../../assets/img/nlp/02/img_3.png" width="100%" height="100%" /></p>
<ul>
  <li>K는 Encoder에서 Masking되는 토큰 개수를 결정하는 하이퍼 파라미터</li>
  <li>하이퍼 파라미터 K를 조정함으로써 MASS는 BERT의 Masked LM과 GPT의 Standard LM을 모두 구현 가능</li>
  <li>BERT Masked LM
    <ul>
      <li>K=1일 때, Encoder에서는 하나의 토큰이 Masking되고, Decoder는 Masking된 하나의 토큰을 예측해야 하므로 BERT의 Masked LM과 같아짐</li>
    </ul>
  </li>
  <li>GPT Standard LM
    <ul>
      <li>하이퍼 파라미터 K가 Input 문장의 전체 길이인 m과 같을 때 Encoder의 모든 토큰들이 Masking됨</li>
      <li>Decoder는 Encoder로부터 어떠한 정보도 부여 받지 않은 채 모든 토큰을 예측해야 하므로, GPT의 Standard LM과 같아짐</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/02/img_4.png" width="100%" height="100%" /></p>
<ul>
  <li>m - Input sequence의 전체길이</li>
  <li>u - Maskinge된 Fragment의 시작점</li>
  <li>v - Masking된 Fragment의 끝점</li>
  <li>X^u - u부터 v까지의 fragment</li>
  <li>X^\u:v - u부터 v까지 Making된 Input Sequence</li>
</ul>

<p><br /><br />
#Experiments</p>
<ul>
  <li>Model Configuration
    <ul>
      <li>1024 embeding/hidden size, 4096 feed-forward filter size를 가진 6-layer encoder, 6-layer decoder로 구성된 기본 모델 구조 Transformer</li>
    </ul>
  </li>
  <li>Datasets
    <ul>
      <li>2007 - 2017 WMT News Crawl datasets 190M English, 62M French, German 270M</li>
      <li>MASS의 효과 검증을 위한 low-resource language Romanian</li>
    </ul>
  </li>
  <li>Pre-training Details
    <ul>
      <li>Fragment length K의 길이를 문장에 있는 총 토큰 수의 약 50%로 설정하고 정확성의 변화를 비교하기 위하여 K를 바꿔가며 실험</li>
    </ul>
  </li>
</ul>

<p><br /><br />
#Experiments:Unsupervised Machine Translation
<img src="../../assets/img/nlp/02/img_5.png" width="100%" height="100%" /></p>

:ET