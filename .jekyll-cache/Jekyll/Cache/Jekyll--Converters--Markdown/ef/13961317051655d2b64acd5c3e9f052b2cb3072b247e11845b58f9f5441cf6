I"q<h1 id="introduction">Introduction</h1>
<ul>
  <li>Multi-Task Learning(MTL)은 새로운 Task를 학습하는 데 도움이 되도록 이전 작업에서 학습된 지식을 적용하는 인간 학습 활동에서 영감을 받음</li>
  <li>Deep neural networks(DNN)를 이용한 representation learning에 MTL을 적용하는 것에 대한 관심이 높아지고 있음
    <ul>
      <li>DNN을 이용한 representation learning은 많은 양의 데이터를 요구함, MTL은 많은 task에서의 supervised labeled data를 제공함</li>
      <li>MTL은 특정 Task에 Overfitting 되지 않도록 Regularization 효과를 줌</li>
    </ul>
  </li>
  <li>MTL과 대조적으로, Language Model은 대용량의 unsupervised dataset을 활용하여 모델을 학습함
    <ul>
      <li>ELMo, GPT, BERT</li>
    </ul>
  </li>
  <li>MT-DNN은 Language Model Pre-Training을 활용한 BERT에 Multi-task learning을 적용하여 성능을 개선한 모델
<br /><br />
    <h1 id="tasks">Tasks</h1>
  </li>
  <li>GLUE의 9개 task를 MTL에 활용</li>
  <li>Single Sentence Classification
    <ul>
      <li>하나의 문장이 주어졌을 때 문장의 Class를 분류하는 Task</li>
      <li>CoLA : 문장이 문법적으로 맞는지 분류 (True/False)</li>
      <li>SST-2 : 영화 Review 문장의 감정 분류 (Poistive/Negative)</li>
    </ul>
  </li>
  <li>Text Similarity
    <ul>
      <li>문장 쌍이 주어졌을 때, 점수를 예측하는 Regression Task</li>
      <li>STB-B : 문장 간의 의미적 유사도를 점수로 예측</li>
    </ul>
  </li>
  <li>Pairwise Text Classification
    <ul>
      <li>문장 쌍이 주어졌을 때, 문장의 관계를 분류하는 Task</li>
      <li>RTE, MNLI : 문장 간의 의미적 관계를 3가지로 분류 (Entailment, Contradiction, Neutral)
– QQP, MRPC : 문장 간 의미가 같음 여부를 분류 (True/False)
– Relevance Ranking
– QNLI : 질문과 해당 지문 중 한 문장이 쌍으로 주어졌을 때 해당 지문 문장에 질문의 답이 있는지 여부를 분류 (True/False)
– MT-DNN에서는 이를 Rank 방식으로 바꾸어 모든 지문 문장에 정답이 있을 가능성을 Scoring 하여 가장 Score가 높은 지문 문장을 True로 분류하는 방식으로 Task 수행
<br /><br />
        <h1 id="model">Model</h1>
      </li>
    </ul>
  </li>
  <li>ELMO
    <ul>
      <li>ELMO word representations are functions of the entire input sentence.</li>
      <li>They are computed on top of two-layer biLMs with character convolutions, as a linear function of the internal network states.</li>
      <li>This setup allows us to do semi-supervised learning, where the biLM is pre-trained at a large scale and easily incorporated into a wide range of existing neural NLP architectures.</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/03/img.png" width="100%" height="100%" /></p>

<ul>
  <li>Pretrained BiLM</li>
</ul>

<p><img src="../../assets/img/nlp/03/img_1.png" width="100%" height="100%" />
<img src="../../assets/img/nlp/03/img_2.png" width="100%" height="100%" /></p>

<ul>
  <li>Task-specific ELMo Embedding</li>
</ul>

<p><img src="../../assets/img/nlp/03/img_3.png" width="100%" height="100%" /></p>

<ul>
  <li>Using biLMs for supervised NLP tasks
    <ul>
      <li>기존의 임베딩 벡터와 함께 사용된다.</li>
      <li>ELMo 표현을 만드는데 사용된 사전 훈련된 언어 모델의 가중치는 고정시키고, 각 층의 가중치와 스칼라 파라미터는 훈련 과정에서 학습된다.</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/03/img_4.png" width="100%" height="100%" /></p>

<p><br /><br /></p>
<h1 id="evaluation">Evaluation</h1>

<p><img src="../../assets/img/nlp/03/img_5.png" width="100%" height="100%" /></p>

<ul>
  <li>6개의 NLP task에서 에러율을 6~20% 줄였다.</li>
  <li>6개의 NLP task에서 높은 점수를 기록했다.</li>
</ul>

<p><br /><br /></p>

<h1 id="analysis">Analysis</h1>

<p><img src="../../assets/img/nlp/03/img_6.png" width="100%" height="100%" /></p>
<ul>
  <li>기존에 top layer output만 사용 한 것 대비 성능 향상을 검증했다.</li>
  <li>대부분의 경우 Regularization parameter λ 가 작을수록 성능이 더 좋아지는 경향이 있다.</li>
</ul>

<p><br />
<img src="../../assets/img/nlp/03/img_7.png" width="100%" height="100%" /></p>
<ul>
  <li>일부 task(SNLI, SQuAD) 에서는 ELMo 벡터를 Output에 다시 concat 시키는 것이 효과가 있다.</li>
</ul>

<p><br />
<img src="../../assets/img/nlp/03/img_8.png" width="100%" height="100%" /></p>
<ul>
  <li>GloVe 단어 벡터에서 ‘play’와 비슷한 단어는 품사를 변형한 것 또는 스포츠에 관한 유사 단어만 뜬다.</li>
  <li>biLM에서는 문맥을 고려한다는 것을 알 수 있다.</li>
</ul>

<p><br />
<img src="../../assets/img/nlp/03/img_9.png" width="100%" height="100%" /></p>
<ul>
  <li>biLM의 첫 번째 레이어는 syntactic 정보를, 두 번째 레이어는 semantic 정보를 더 잘 인코딩 하는 것으로 나타난다.</li>
  <li>이는 biLM의 모든 레이어를 사용하는 것이 성능향상에 도움이 된다는 것을 증명한다.</li>
</ul>

<p><br />
<img src="../../assets/img/nlp/03/img_10.png" width="100%" height="100%" /></p>
<ul>
  <li>ELMo를 활용하면 같은 성능을 내는 데에 있어 훨씬 학습이 효율적임을 알 수 있다.</li>
</ul>

<p><br /><br /></p>
<h1 id="conclusion">Conclusion</h1>
<ul>
  <li>We have introduced a general approach for learning high-quality deep context-dependent representations from biLMs, and shown large improvements when applying ELMo to a broad range of NLP tasks.</li>
  <li>Through ablations and other controlled experiments, we have also confirmed that the biLM layers efficiently encode different types of syntactic and semantic information about words in-context, and that using all layers improves overall task performance.</li>
</ul>
:ET