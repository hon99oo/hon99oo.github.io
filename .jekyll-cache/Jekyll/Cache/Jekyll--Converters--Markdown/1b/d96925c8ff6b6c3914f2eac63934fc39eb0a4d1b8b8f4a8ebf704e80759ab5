I"7<h1 id="introduction">Introduction</h1>
<ul>
  <li>Pre-trained word representations are a key component in many neural language understanding models.</li>
  <li>Our representations differ from traditional word type embeddings in that each token is assigned a representation that is a function of the entire input sentence.</li>
  <li>We use vectors derived from a bidirectional LSTM that is trained with a coupled language model (LM) objective on a large text corpus.</li>
  <li>In many experiments, the ELMo representation has been shown to be very excellent, and the error rate is relatively reduced by 20%
    <h1 id="related-work">Related Work</h1>
    <h1 id="elmo-model">ELMo model</h1>
    <h1 id="evaluation">Evaluation</h1>
    <h1 id="analysis">Analysis</h1>
    <h1 id="conclusion">Conclusion</h1>
  </li>
</ul>
:ET