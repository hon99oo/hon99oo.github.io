I"Ò<h1 id="introduction">Introduction</h1>
<ul>
  <li>Multi-Task Learning(MTL)ì€ ìƒˆë¡œìš´ Taskë¥¼ í•™ìŠµí•˜ëŠ” ë° ë„ì›€ì´ ë˜ë„ë¡ ì´ì „ ì‘ì—…ì—ì„œ í•™ìŠµëœ ì§€ì‹ì„ ì ìš©í•˜ëŠ” ì¸ê°„ í•™ìŠµ í™œë™ì—ì„œ ì˜ê°ì„ ë°›ìŒ</li>
  <li>Deep neural networks(DNN)ë¥¼ ì´ìš©í•œ representation learningì— MTLì„ ì ìš©í•˜ëŠ” ê²ƒì— ëŒ€í•œ ê´€ì‹¬ì´ ë†’ì•„ì§€ê³  ìˆìŒ
    <ul>
      <li>DNNì„ ì´ìš©í•œ representation learningì€ ë§ì€ ì–‘ì˜ ë°ì´í„°ë¥¼ ìš”êµ¬í•¨, MTLì€ ë§ì€ taskì—ì„œì˜ supervised labeled dataë¥¼ ì œê³µí•¨</li>
      <li>MTLì€ íŠ¹ì • Taskì— Overfitting ë˜ì§€ ì•Šë„ë¡ Regularization íš¨ê³¼ë¥¼ ì¤Œ</li>
    </ul>
  </li>
  <li>MTLê³¼ ëŒ€ì¡°ì ìœ¼ë¡œ, Language Modelì€ ëŒ€ìš©ëŸ‰ì˜ unsupervised datasetì„ í™œìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•¨
    <ul>
      <li>ELMo, GPT, BERT</li>
    </ul>
  </li>
  <li>MT-DNNì€ Language Model Pre-Trainingì„ í™œìš©í•œ BERTì— Multi-task learningì„ ì ìš©í•˜ì—¬ ì„±ëŠ¥ì„ ê°œì„ í•œ ëª¨ë¸
<br /><br />
    <h1 id="related-works">Related Works</h1>
  </li>
  <li>Pretrained word vectorì˜ í™œìš©ì´ í‘œì¤€í™” ë˜ì—ˆì§€ë§Œ, í•˜ë‚˜ì˜ ë‹¨ì–´ì— í•˜ë‚˜ì˜ ë²¡í„°ë¥¼ ë¶€ì—¬í•˜ë‹¤ë³´ë‹ˆ context-independentí•œ ë¬¸ì œê°€ ìˆì—ˆë‹¤.</li>
  <li>ì›Œë“œ ì„ë² ë”©ì„ í’ë¶€í•˜ê²Œ í•˜ê¸° ìœ„í•´, subword informationì„ í™œìš©í•˜ê±°ë‚˜ ë‹¤ì˜ì–´ì˜ ê²½ìš° ì˜ë¯¸ë³„ë¡œ ë‹¤ë¥¸ ë²¡í„°ë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ë°©ë²•ì´ ë“±ì¥í•˜ì˜€ë‹¤.
    <ul>
      <li>context2vec</li>
      <li>CoVe</li>
    </ul>
  </li>
  <li>ì´ì „ ì—°êµ¬ì— ì˜í•˜ë©´ biRNNì˜ ì„œë¡œ ë‹¤ë¥¸ ë ˆì´ì–´ê°€ ë‹¤ë¥¸ í˜•íƒœì˜ ì •ë³´ë¥¼ ì¸ì½”ë”©í•˜ëŠ”ë°, ë³¸ ì—°êµ¬ì—ì„œë„ ìœ ì‚¬í•œ íš¨ê³¼ê°€ ë‚˜íƒ€ë‚¬ë‹¤.</li>
</ul>

<p><br /><br /></p>
<h1 id="model">Model</h1>
<ul>
  <li>ELMO
    <ul>
      <li>ELMO word representations are functions of the entire input sentence.</li>
      <li>They are computed on top of two-layer biLMs with character convolutions, as a linear function of the internal network states.</li>
      <li>This setup allows us to do semi-supervised learning, where the biLM is pre-trained at a large scale and easily incorporated into a wide range of existing neural NLP architectures.</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/03/img.png" width="100%" height="100%" /></p>

<ul>
  <li>Pretrained BiLM</li>
</ul>

<p><img src="../../assets/img/nlp/03/img_1.png" width="100%" height="100%" />
<img src="../../assets/img/nlp/03/img_2.png" width="100%" height="100%" /></p>

<ul>
  <li>Task-specific ELMo Embedding</li>
</ul>

<p><img src="../../assets/img/nlp/03/img_3.png" width="100%" height="100%" /></p>

<ul>
  <li>Using biLMs for supervised NLP tasks
    <ul>
      <li>ê¸°ì¡´ì˜ ì„ë² ë”© ë²¡í„°ì™€ í•¨ê»˜ ì‚¬ìš©ëœë‹¤.</li>
      <li>ELMo í‘œí˜„ì„ ë§Œë“œëŠ”ë° ì‚¬ìš©ëœ ì‚¬ì „ í›ˆë ¨ëœ ì–¸ì–´ ëª¨ë¸ì˜ ê°€ì¤‘ì¹˜ëŠ” ê³ ì •ì‹œí‚¤ê³ , ê° ì¸µì˜ ê°€ì¤‘ì¹˜ì™€ ìŠ¤ì¹¼ë¼ íŒŒë¼ë¯¸í„°ëŠ” í›ˆë ¨ ê³¼ì •ì—ì„œ í•™ìŠµëœë‹¤.</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/03/img_4.png" width="100%" height="100%" /></p>

<p><br /><br /></p>
<h1 id="evaluation">Evaluation</h1>

<p><img src="../../assets/img/nlp/03/img_5.png" width="100%" height="100%" /></p>

<ul>
  <li>6ê°œì˜ NLP taskì—ì„œ ì—ëŸ¬ìœ¨ì„ 6~20% ì¤„ì˜€ë‹¤.</li>
  <li>6ê°œì˜ NLP taskì—ì„œ ë†’ì€ ì ìˆ˜ë¥¼ ê¸°ë¡í–ˆë‹¤.</li>
</ul>

<p><br /><br /></p>

<h1 id="analysis">Analysis</h1>

<p><img src="../../assets/img/nlp/03/img_6.png" width="100%" height="100%" /></p>
<ul>
  <li>ê¸°ì¡´ì— top layer outputë§Œ ì‚¬ìš© í•œ ê²ƒ ëŒ€ë¹„ ì„±ëŠ¥ í–¥ìƒì„ ê²€ì¦í–ˆë‹¤.</li>
  <li>ëŒ€ë¶€ë¶„ì˜ ê²½ìš° Regularization parameter Î» ê°€ ì‘ì„ìˆ˜ë¡ ì„±ëŠ¥ì´ ë” ì¢‹ì•„ì§€ëŠ” ê²½í–¥ì´ ìˆë‹¤.</li>
</ul>

<p><br />
<img src="../../assets/img/nlp/03/img_7.png" width="100%" height="100%" /></p>
<ul>
  <li>ì¼ë¶€ task(SNLI, SQuAD) ì—ì„œëŠ” ELMo ë²¡í„°ë¥¼ Outputì— ë‹¤ì‹œ concat ì‹œí‚¤ëŠ” ê²ƒì´ íš¨ê³¼ê°€ ìˆë‹¤.</li>
</ul>

<p><br />
<img src="../../assets/img/nlp/03/img_8.png" width="100%" height="100%" /></p>
<ul>
  <li>GloVe ë‹¨ì–´ ë²¡í„°ì—ì„œ â€˜playâ€™ì™€ ë¹„ìŠ·í•œ ë‹¨ì–´ëŠ” í’ˆì‚¬ë¥¼ ë³€í˜•í•œ ê²ƒ ë˜ëŠ” ìŠ¤í¬ì¸ ì— ê´€í•œ ìœ ì‚¬ ë‹¨ì–´ë§Œ ëœ¬ë‹¤.</li>
  <li>biLMì—ì„œëŠ” ë¬¸ë§¥ì„ ê³ ë ¤í•œë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
</ul>

<p><br />
<img src="../../assets/img/nlp/03/img_9.png" width="100%" height="100%" /></p>
<ul>
  <li>biLMì˜ ì²« ë²ˆì§¸ ë ˆì´ì–´ëŠ” syntactic ì •ë³´ë¥¼, ë‘ ë²ˆì§¸ ë ˆì´ì–´ëŠ” semantic ì •ë³´ë¥¼ ë” ì˜ ì¸ì½”ë”© í•˜ëŠ” ê²ƒìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤.</li>
  <li>ì´ëŠ” biLMì˜ ëª¨ë“  ë ˆì´ì–´ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì„±ëŠ¥í–¥ìƒì— ë„ì›€ì´ ëœë‹¤ëŠ” ê²ƒì„ ì¦ëª…í•œë‹¤.</li>
</ul>

<p><br />
<img src="../../assets/img/nlp/03/img_10.png" width="100%" height="100%" /></p>
<ul>
  <li>ELMoë¥¼ í™œìš©í•˜ë©´ ê°™ì€ ì„±ëŠ¥ì„ ë‚´ëŠ” ë°ì— ìˆì–´ í›¨ì”¬ í•™ìŠµì´ íš¨ìœ¨ì ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.</li>
</ul>

<p><br /><br /></p>
<h1 id="conclusion">Conclusion</h1>
<ul>
  <li>We have introduced a general approach for learning high-quality deep context-dependent representations from biLMs, and shown large improvements when applying ELMo to a broad range of NLP tasks.</li>
  <li>Through ablations and other controlled experiments, we have also confirmed that the biLM layers efficiently encode different types of syntactic and semantic information about words in-context, and that using all layers improves overall task performance.</li>
</ul>
:ET