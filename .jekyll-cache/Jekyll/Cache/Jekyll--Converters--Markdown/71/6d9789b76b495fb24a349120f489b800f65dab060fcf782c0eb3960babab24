I"4<h1>Introduction</h1>
<ul>
  <li>Neural Machine Translation
    <ul>
      <li>End-to-End 학습 접근 방식의 자동 번역</li>
      <li>기존 구문 기반 번역의 약점을 극복</li>
    </ul>
  </li>
  <li>Neural Machine Translation 단점
    <ul>
      <li>데이터 양과 매개변수가 많아 훈련과 추론 속도가 느림</li>
      <li>Rare Word 처리의 문제점</li>
      <li>가끔씩 모든 단어에 대해 번역하지 못함</li>
    </ul>
  </li>
  <li>Google’s Neural Machine Translation
    <ul>
      <li>LSTM으로 이루어져 있는 8개의 ENCODER와 8개의 DECODER</li>
      <li>병렬 처리 개선을 위해 DECODER의 최하층과 ENCODER의 최상층을 ATTENTION으로 연결</li>
      <li>번역속도를 높이기 위해 low-precision arithmetic, Rare Word 처리를 위해 WordPiece 사용</li>
    </ul>
  </li>
</ul>
<hr />

<h1>Model Architecture</h1>
<p><img src="../../assets/img/nlp/nlp-01-01.png" width="100%" height="100%" /></p>
<ul>
  <li>Model Parallelism
    <ul>
      <li>모델 병렬화와 데이터 병렬화 모두 사용함</li>
      <li>Downpour SGD를 사용하여 데이터 병렬화
        <ul>
          <li>여러개의 모델로 나누어, 여러개의 머신에서 동시에 학습함, 각각 학습된 gradient를 평균내어 모델에 적용</li>
          <li>실험에서는 10개의 머신에서 128개의 문장을 Mini-batch로 사용</li>
        </ul>
      </li>
      <li>모델 병렬화
        <ul>
          <li>머신당 8개의 GPU 사용 ( 각 층 마다 서로 다른 GPU에 할당 )</li>
          <li>i번째 레이어의 작업이 종료전에 i+1번째 작업 진행 가능</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>
<hr />

<h1>Segmentation Approches</h1>
<ul>
  <li>Wordpiece Model
    <ul>
      <li>띄어쓰기는 _, 단어는 내부단어 통계에 기반하여 띄어쓰기로 분리</li>
      <li>띄어쓰기를 _로 치환한 이유는 차후에 문장 복원을 위해</li>
      <li>실험에서는 wordpiece를 8K~32K에서 좋은 결과 얻음</li>
      <li>wordpiece로도 얻을 수 없었던 rare word는 copy model을 사용</li>
    </ul>
  </li>
</ul>

<p><img src="../../assets/img/nlp/nlp-01-02.png" width="100%" height="100%" /></p>

<ul>
  <li>Mixed Word/Character Model
    <ul>
      <li>OOV 처리를 <UNK>로 하지 않고 문자 단위로 나누어 처리함</UNK></li>
      <li>시작 문자 <b>, 중간 문자 <M>, 끝 문자 <E></E></M></b></li>
      <li>전체 작업 과정에서 유지한 채로 학습한 후 태그를 삭제함</li>
    </ul>
  </li>
</ul>

<hr />

<h1>Training Criteria</h1>
<ul>
  <li>Maximum-liklihood 학습 방식은 로그 확률 값을 최대화하는 목적 함수 ( BLUE 평가 지표와 부합되지 않음 )</li>
</ul>

<p><img src="../../assets/img/nlp/nlp-01-03.png" width="100%" height="100%" /></p>
<ul>
  <li>Reward개념의 목적함수 사용</li>
</ul>

<p><img src="../../assets/img/nlp/nlp-01-04.png" width="100%" height="100%" /></p>
<ul>
  <li>r은 문장 단위 점수 ( 출력 문서와 실제 문서의 차이 계산 )</li>
  <li>GLEU 점수 지표 사용 ( 출력 문장과 정답 문장을 1~4 토큰으로 만든 뒤 recall과 precision을 구한 뒤 더 작은 값을 GLEU로 정함 )</li>
  <li>ML방식과 RL 방식 혼합하여 사용 이 때, a는 0.017</li>
</ul>
:ET